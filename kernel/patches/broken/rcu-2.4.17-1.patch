diff -urN linux-2.4.17/arch/i386/kernel/apic.c linux-2.4.17-rcu-1/arch/i386/kernel/apic.c
--- linux-2.4.17/arch/i386/kernel/apic.c	Sat Nov 10 03:42:55 2001
+++ linux-2.4.17-rcu-1/arch/i386/kernel/apic.c	Fri Jan 11 18:05:20 2002
@@ -23,6 +23,7 @@
 #include <linux/interrupt.h>
 #include <linux/mc146818rtc.h>
 #include <linux/kernel_stat.h>
+#include <linux/rcudata.h>
 
 #include <asm/atomic.h>
 #include <asm/smp.h>
@@ -958,6 +959,7 @@
 	int user = user_mode(regs);
 	int cpu = smp_processor_id();
 
+	RCU_PROCESS_CALLBACKS(cpu_number_map(cpu), regs);
 	/*
 	 * The profiling function is SMP safe. (nothing can mess
 	 * around with "current", and the profiling counters are
diff -urN linux-2.4.17/include/linux/rcudata.h linux-2.4.17-rcu-1/include/linux/rcudata.h
--- linux-2.4.17/include/linux/rcudata.h	Thu Jan  1 05:30:00 1970
+++ linux-2.4.17-rcu-1/include/linux/rcudata.h	Fri Jan 11 18:05:20 2002
@@ -0,0 +1,126 @@
+/*
+ * Read-Copy Update mechanism for mutual exclusion 
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright (c) International Business Machines Corp., 2001
+ *
+ * Author: Dipankar Sarma <dipankar@in.ibm.com>
+ * 
+ * Based on the original work by Paul McKenney <paul.mckenney@us.ibm.com>
+ * Papers:
+ * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf
+ * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)
+ *
+ * For detailed explanation of Read-Copy Update mechanism see -
+ * 		http://lse.sourceforge.net/locking/rcupdate.html
+ *
+ */
+
+#ifndef __LINUX_RCUDATA_H
+#define __LINUX_RCUDATA_H
+
+#include <linux/cache.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/rcupdate.h>
+
+/* Batch counter type. */
+typedef long rcu_batch_t;
+
+/* Control variables for rcupdate callback mechanism. */
+typedef struct rcu_ctrlblk {
+	spinlock_t	mutex;		/* Guard this struct                  */
+	rcu_batch_t	curbatch;	/* Current batch number.	      */
+	rcu_batch_t	maxbatch;	/* Max requested batch number.        */
+	unsigned long	rcu_cpu_mask; 	/* CPUs that need to switch in order  */
+					/* for current batch to proceed.      */
+} rcu_ctrlblk_t;
+
+/*
+ * RCU_BATCH_XX(rcu_batch_t a, rcu_batch_t b)
+ * 
+ * Returns true if batch number ``a'' compares as specified to ``b''.
+ * This comparison allows for the fact that the batch numbers wrap.
+ */
+#define RCU_BATCH_EQ(a, b)		((a) - (b) == 0)
+#define RCU_BATCH_GE(a, b)		((a) - (b) >= 0)
+#define RCU_BATCH_GT(a, b)		((a) - (b) > 0)
+#define RCU_BATCH_LE(a, b)		((a) - (b) <= 0)
+#define RCU_BATCH_LT(a, b)		((a) - (b) < 0)
+#define RCU_BATCH_NE(a, b)		((a) - (b) != 0)
+
+/*
+ * Per-CPU data for Read-Copy UPdate.
+ * We maintain callbacks in 3 per-cpu lists. Each list represents
+ * a batch and every batch is given a number that is global across
+ * all CPUs. The global current batch number (curbatch) represents
+ * the current batch of callbacks for which the quiescent cycle
+ * has started.
+ * nxtlist - new callbacks are added here
+ * curlist - current batch for which quiescent cycle started if any
+ */
+struct rcu_data {
+	/*
+	 * Per-cpu counters maintained to indicate quiscent state
+	 * transition. A queiscent state can be context switch,
+	 * idle loop or user mode code. A quiesent state transition
+	 * indicates that the CPU no longer has kernel data references.
+	 */
+	long		qsctr;		/* User-mode/idle loop etc. */
+
+        /*
+         * Per-CPU variables used by the write side of the read-copy update
+         * mechanism.  See kernel/rcupdate.c.
+         */
+        long              last_qsctr;	 /* value of qsctr at beginning */
+                                         /* of last rcu interval. */
+        rcu_batch_t       batch;         /* Batch # for current RCU batch */
+
+        struct list_head  nxtlist;
+        struct list_head  curlist;
+	struct tasklet_struct  tasklet;
+} ____cacheline_aligned_in_smp;
+
+extern struct rcu_data rcu_data[NR_CPUS];
+
+#define RCU_qsctr(cpu) 		(rcu_data[(cpu)].qsctr)
+#define RCU_last_qsctr(cpu) 	(rcu_data[(cpu)].last_qsctr)
+#define RCU_batch(cpu) 		(rcu_data[(cpu)].batch)
+#define RCU_nxtlist(cpu) 	(rcu_data[(cpu)].nxtlist)
+#define RCU_curlist(cpu) 	(rcu_data[(cpu)].curlist)
+#define RCU_tasklet(cpu) 	(rcu_data[(cpu)].tasklet)
+
+#define RCU_QSCTR_INVALID	0
+
+/*
+ * This depends on tasklets being per-CPU. Also is intended to be
+ * called with irqs disabled so that no tasklet can run on that
+ * CPU - this ensures that there is no race between checking
+ * the state and invoking the tasklet.
+ */
+#define RCU_PROCESS_CALLBACKS(cpu,regs) \
+	do { \
+		if (user_mode(regs) || current == init_tasks[cpu]) \
+			RCU_qsctr(cpu)++; \
+		if ((RCU_tasklet(cpu).state & \
+		    ((1 << TASKLET_STATE_SCHED) | (1 << TASKLET_STATE_RUN))) \
+									== 0) \
+			tasklet_schedule(&RCU_tasklet(cpu)); \
+	} while(0)
+
+extern void rcu_init(void);
+
+#endif
diff -urN linux-2.4.17/include/linux/rcupdate.h linux-2.4.17-rcu-1/include/linux/rcupdate.h
--- linux-2.4.17/include/linux/rcupdate.h	Thu Jan  1 05:30:00 1970
+++ linux-2.4.17-rcu-1/include/linux/rcupdate.h	Fri Jan 11 18:05:20 2002
@@ -0,0 +1,48 @@
+/*
+ * Read-Copy Update mechanism for mutual exclusion 
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright (c) International Business Machines Corp., 2001
+ *
+ * Author: Dipankar Sarma <dipankar@in.ibm.com>
+ * 
+ * Based on the original work by Paul McKenney <paul.mckenney@us.ibm.com>
+ * Papers:
+ * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf
+ * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)
+ *
+ * For detailed explanation of Read-Copy Update mechanism see -
+ * 		http://lse.sourceforge.net/locking/rcupdate.html
+ *
+ */
+
+#ifndef __LINUX_RCUPDATE_H
+#define __LINUX_RCUPDATE_H
+
+#include <linux/list.h>
+
+/*
+ * Callback structure for use with call_rcu(). 
+ */
+struct rcu_head {
+	struct list_head list;
+	void (*func)(void *obj);
+	void *arg;
+};
+
+extern void call_rcu(struct rcu_head *head, void (*func)(void *arg), void *arg);
+
+#endif /* __LINUX_RCUPDATE_H */
diff -urN linux-2.4.17/init/main.c linux-2.4.17-rcu-1/init/main.c
--- linux-2.4.17/init/main.c	Fri Dec 21 23:12:04 2001
+++ linux-2.4.17-rcu-1/init/main.c	Fri Jan 11 18:05:20 2002
@@ -27,6 +27,7 @@
 #include <linux/iobuf.h>
 #include <linux/bootmem.h>
 #include <linux/tty.h>
+#include <linux/rcudata.h>
 
 #include <asm/io.h>
 #include <asm/bugs.h>
@@ -554,6 +555,7 @@
 	printk("Kernel command line: %s\n", saved_command_line);
 	parse_options(command_line);
 	trap_init();
+	rcu_init();
 	init_IRQ();
 	sched_init();
 	softirq_init();
diff -urN linux-2.4.17/kernel/Makefile linux-2.4.17-rcu-1/kernel/Makefile
--- linux-2.4.17/kernel/Makefile	Mon Sep 17 09:52:40 2001
+++ linux-2.4.17-rcu-1/kernel/Makefile	Fri Jan 11 18:05:20 2002
@@ -9,12 +9,12 @@
 
 O_TARGET := kernel.o
 
-export-objs = signal.o sys.o kmod.o context.o ksyms.o pm.o exec_domain.o printk.o
+export-objs = signal.o sys.o kmod.o context.o ksyms.o pm.o exec_domain.o printk.o rcupdate.o
 
 obj-y     = sched.o dma.o fork.o exec_domain.o panic.o printk.o \
 	    module.o exit.o itimer.o info.o time.o softirq.o resource.o \
 	    sysctl.o acct.o capability.o ptrace.o timer.o user.o \
-	    signal.o sys.o kmod.o context.o
+	    signal.o sys.o kmod.o context.o rcupdate.o
 
 obj-$(CONFIG_UID16) += uid16.o
 obj-$(CONFIG_MODULES) += ksyms.o
diff -urN linux-2.4.17/kernel/rcupdate.c linux-2.4.17-rcu-1/kernel/rcupdate.c
--- linux-2.4.17/kernel/rcupdate.c	Thu Jan  1 05:30:00 1970
+++ linux-2.4.17-rcu-1/kernel/rcupdate.c	Fri Jan 11 18:05:20 2002
@@ -0,0 +1,211 @@
+/*
+ * Read-Copy Update mechanism for mutual exclusion
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright (c) International Business Machines Corp., 2001
+ *
+ * Author: Dipankar Sarma <dipankar@in.ibm.com>
+ * 
+ * Based on the original work by Paul McKenney <paul.mckenney@us.ibm.com>
+ * Papers:
+ * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf
+ * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)
+ *
+ * For detailed explanation of Read-Copy Update mechanism see -
+ * 		http://lse.sourceforge.net/locking/rcupdate.html
+ *
+ */
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/smp.h>
+#include <linux/sched.h>
+#include <asm/atomic.h>
+#include <asm/bitops.h>
+#include <linux/module.h>
+#include <linux/rcudata.h>
+
+/* Definition for rcupdate control block. */
+static rcu_ctrlblk_t rcu_ctrlblk = 
+		        { SPIN_LOCK_UNLOCKED,
+			  1, 1,				/* Generations*/
+			  0				/* Mask     */
+			};
+struct rcu_data rcu_data[NR_CPUS];
+
+static void rcu_reg_batch(rcu_batch_t newbatch);
+
+/*
+ * Register a new rcu callback. This will be invoked as soon
+ * as all CPUs have performed a context switch or been seen in the
+ * idle loop or in a user process.
+ */
+void call_rcu(struct rcu_head *head, void (*func)(void *arg), void *arg)
+{
+	int cpu = cpu_number_map(smp_processor_id());
+
+	head->func = func;
+	head->arg = arg;
+	local_bh_disable();
+	list_add_tail(&head->list, &RCU_nxtlist(cpu));
+	local_bh_enable();
+}
+
+/*
+ * Invoke the completed RCU callbacks. They are expected to be in
+ * a per-cpu list.
+ */
+static void rcu_invoke_callbacks(struct list_head *list)
+{
+	struct list_head *entry;
+	struct rcu_head *head;
+
+	while (!list_empty(list)) {
+		entry = list->next;
+		list_del(entry);
+		head = list_entry(entry, struct rcu_head, list);
+		head->func(head->arg);
+	}
+}
+
+/*
+ * Register a new batch of callbacks, and start it up if there is currently no
+ * active batch and the batch to be registered has not already occurred.
+ * Caller must hold the rcu_ctrlblk lock.
+ */
+static void rcu_reg_batch(rcu_batch_t newbatch)
+{
+	if (RCU_BATCH_LT(rcu_ctrlblk.maxbatch, newbatch)) {
+		rcu_ctrlblk.maxbatch = newbatch;
+	}
+	if (RCU_BATCH_LT(rcu_ctrlblk.maxbatch, rcu_ctrlblk.curbatch) ||
+	    (rcu_ctrlblk.rcu_cpu_mask != 0)) {
+		return;
+	}
+	rcu_ctrlblk.rcu_cpu_mask = cpu_online_map;
+}
+
+/*
+ * Check if the cpu has gone through a quiescent state (say context
+ * switch). If so and if it already hasn't done so in this RCU
+ * quiescent cycle, then indicate that it has done so.
+ */
+static void rcu_check_quiescent_state(void)
+{
+	int cpu = cpu_number_map(smp_processor_id());
+
+	if (!test_bit(cpu, &rcu_ctrlblk.rcu_cpu_mask)) {
+		return;
+	}
+
+	/* 
+	 * Races with local timer interrupt - in the worst case
+	 * we may miss one quiescent state of that CPU. That is
+	 * tolerable. So no need to disable interrupts.
+	 */
+	if (RCU_last_qsctr(cpu) == RCU_QSCTR_INVALID) {
+		RCU_last_qsctr(cpu) = RCU_qsctr(cpu);
+		return;
+	}
+	if (RCU_qsctr(cpu) == RCU_last_qsctr(cpu)) {
+		return;
+	}
+
+	spin_lock(&rcu_ctrlblk.mutex);
+	if (!test_bit(cpu, &rcu_ctrlblk.rcu_cpu_mask)) {
+		spin_unlock(&rcu_ctrlblk.mutex);
+		return;
+	}
+	clear_bit(cpu, &rcu_ctrlblk.rcu_cpu_mask);
+	RCU_last_qsctr(cpu) = RCU_QSCTR_INVALID;
+	if (rcu_ctrlblk.rcu_cpu_mask != 0) {
+		spin_unlock(&rcu_ctrlblk.mutex);
+		return;
+	}
+	rcu_ctrlblk.curbatch++;
+	rcu_reg_batch(rcu_ctrlblk.maxbatch);
+	spin_unlock(&rcu_ctrlblk.mutex);
+}
+
+
+/*
+ * The caller must disable interrupts and must have verified that there is
+ * a rcu callback list to be handled and that this CPU has its
+ * bit set.
+ */
+static void rcu_check_callbacks(void)
+{
+	int cpu = cpu_number_map(smp_processor_id());
+	LIST_HEAD(list);
+
+	if (!list_empty(&RCU_curlist(cpu)) &&
+	    RCU_BATCH_GT(rcu_ctrlblk.curbatch, RCU_batch(cpu))) {
+		list_splice(&RCU_curlist(cpu), &list);
+		INIT_LIST_HEAD(&RCU_curlist(cpu));
+	}
+
+	if (!list_empty(&RCU_nxtlist(cpu)) && list_empty(&RCU_curlist(cpu))) {
+		list_splice(&RCU_nxtlist(cpu), &RCU_curlist(cpu));
+		INIT_LIST_HEAD(&RCU_nxtlist(cpu));
+
+		/*
+		 * start the next batch of callbacks
+		 */
+		spin_lock(&rcu_ctrlblk.mutex);
+		RCU_batch(cpu) = rcu_ctrlblk.curbatch + 1;
+		rcu_reg_batch(RCU_batch(cpu));
+		spin_unlock(&rcu_ctrlblk.mutex);
+	}
+	rcu_check_quiescent_state();
+	if (!list_empty(&list))
+		rcu_invoke_callbacks(&list);
+}
+
+/*
+ * Look into the per-cpu callback information to see if there is
+ * any processing necessary - if so do it.
+ */
+static void rcu_process_callbacks(unsigned long data)
+{
+	int cpu = cpu_number_map(smp_processor_id());
+
+	if ((!list_empty(&RCU_curlist(cpu)) &&
+	    RCU_BATCH_LT(RCU_batch(cpu), rcu_ctrlblk.curbatch)) ||
+	   (list_empty(&RCU_curlist(cpu)) &&
+				 !list_empty(&RCU_nxtlist(cpu))) ||
+	   test_bit(cpu, &rcu_ctrlblk.rcu_cpu_mask))
+	       rcu_check_callbacks();
+}
+
+/*
+ * Initializes rcu mechanism.  Assumed to be called early.
+ * That is before local timer(SMP) or jiffie timer (uniproc) is setup.
+ * Note that rcu_qsctr and friends are implicitly
+ * initialized due to the choice of ``0'' for RCU_CTR_INVALID.
+ */
+void rcu_init(void)
+{
+	int i;
+
+	memset(&rcu_data[0], 0, sizeof(rcu_data));
+	for (i = 0; i < NR_CPUS; i++) {
+		tasklet_init(&RCU_tasklet(i), rcu_process_callbacks, 0UL);
+		INIT_LIST_HEAD(&RCU_nxtlist(i));
+		INIT_LIST_HEAD(&RCU_curlist(i));
+	}
+}
+
+EXPORT_SYMBOL(call_rcu);
diff -urN linux-2.4.17/kernel/sched.c linux-2.4.17-rcu-1/kernel/sched.c
--- linux-2.4.17/kernel/sched.c	Fri Dec 21 23:12:04 2001
+++ linux-2.4.17-rcu-1/kernel/sched.c	Fri Jan 11 18:05:20 2002
@@ -28,6 +28,7 @@
 #include <linux/kernel_stat.h>
 #include <linux/completion.h>
 #include <linux/prefetch.h>
+#include <linux/rcudata.h>
 #include <linux/compiler.h>
 
 #include <asm/uaccess.h>
@@ -661,6 +662,7 @@
 #endif /* CONFIG_SMP */
 
 	kstat.context_swtch++;
+	RCU_qsctr(this_cpu)++;
 	/*
 	 * there are 3 processes which are affected by a context switch:
 	 *
diff -urN linux-2.4.17/kernel/timer.c linux-2.4.17-rcu-1/kernel/timer.c
--- linux-2.4.17/kernel/timer.c	Mon Oct  8 23:11:41 2001
+++ linux-2.4.17-rcu-1/kernel/timer.c	Fri Jan 11 18:05:20 2002
@@ -22,6 +22,7 @@
 #include <linux/smp_lock.h>
 #include <linux/interrupt.h>
 #include <linux/kernel_stat.h>
+#include <linux/rcudata.h>
 
 #include <asm/uaccess.h>
 
@@ -675,6 +676,7 @@
 {
 	(*(unsigned long *)&jiffies)++;
 #ifndef CONFIG_SMP
+	RCU_PROCESS_CALLBACKS(cpu_number_map(smp_processor_id()), regs);
 	/* SMP process accounting uses the local APIC timer */
 
 	update_process_times(user_mode(regs));
