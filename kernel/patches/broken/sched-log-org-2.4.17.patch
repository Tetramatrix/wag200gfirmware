diff -Nur -X dontdiff linux-2.4.17/arch/i386/kernel/entry.S org/arch/i386/kernel/entry.S
--- linux-2.4.17/arch/i386/kernel/entry.S	Fri Nov  2 20:18:49 2001
+++ org/arch/i386/kernel/entry.S	Tue Mar 19 09:07:47 2002
@@ -618,7 +618,7 @@
 	.long SYMBOL_NAME(sys_madvise)
 	.long SYMBOL_NAME(sys_getdents64)	/* 220 */
 	.long SYMBOL_NAME(sys_fcntl64)
-	.long SYMBOL_NAME(sys_ni_syscall)	/* reserved for TUX */
+	.long SYMBOL_NAME(sys_rdtsc_log)	/* reserved for TUX */
 	.long SYMBOL_NAME(sys_ni_syscall)	/* Reserved for Security */
 	.long SYMBOL_NAME(sys_gettid)
 	.long SYMBOL_NAME(sys_readahead)	/* 225 */
diff -Nur -X dontdiff linux-2.4.17/cpk org/cpk
--- linux-2.4.17/cpk	Wed Dec 31 19:00:00 1969
+++ org/cpk	Mon Apr 22 08:49:51 2002
@@ -0,0 +1,4 @@
+make bzImage 2> err.log
+cp arch/i386/boot/bzImage /boot/vmlinuz-2.4.17-org-log
+lilo
+cat err.log
diff -Nur -X dontdiff linux-2.4.17/drivers/sound/maui_boot.h org/drivers/sound/maui_boot.h
--- linux-2.4.17/drivers/sound/maui_boot.h	Wed Dec 31 19:00:00 1969
+++ org/drivers/sound/maui_boot.h	Thu Oct 17 09:20:03 2002
@@ -0,0 +1,2 @@
+static unsigned char * maui_os = NULL;
+static int maui_osLen = 0;
diff -Nur -X dontdiff linux-2.4.17/drivers/sound/pss_boot.h org/drivers/sound/pss_boot.h
--- linux-2.4.17/drivers/sound/pss_boot.h	Wed Dec 31 19:00:00 1969
+++ org/drivers/sound/pss_boot.h	Thu Oct 17 09:19:47 2002
@@ -0,0 +1,2 @@
+static unsigned char * pss_synth = NULL;
+static int pss_synthLen = 0;
diff -Nur -X dontdiff linux-2.4.17/drivers/sound/trix_boot.h org/drivers/sound/trix_boot.h
--- linux-2.4.17/drivers/sound/trix_boot.h	Wed Dec 31 19:00:00 1969
+++ org/drivers/sound/trix_boot.h	Thu Oct 17 09:19:49 2002
@@ -0,0 +1,2 @@
+static unsigned char * trix_boot = NULL;
+static int trix_boot_len = 0;
diff -Nur -X dontdiff linux-2.4.17/include/asm-i386/unistd.h org/include/asm-i386/unistd.h
--- linux-2.4.17/include/asm-i386/unistd.h	Wed Oct 17 13:03:03 2001
+++ org/include/asm-i386/unistd.h	Tue Mar 19 09:07:55 2002
@@ -5,241 +5,242 @@
  * This file contains the system call numbers.
  */
 
-#define __NR_exit		  1
-#define __NR_fork		  2
-#define __NR_read		  3
-#define __NR_write		  4
-#define __NR_open		  5
-#define __NR_close		  6
-#define __NR_waitpid		  7
-#define __NR_creat		  8
-#define __NR_link		  9
-#define __NR_unlink		 10
-#define __NR_execve		 11
-#define __NR_chdir		 12
-#define __NR_time		 13
-#define __NR_mknod		 14
-#define __NR_chmod		 15
-#define __NR_lchown		 16
-#define __NR_break		 17
-#define __NR_oldstat		 18
-#define __NR_lseek		 19
-#define __NR_getpid		 20
-#define __NR_mount		 21
-#define __NR_umount		 22
-#define __NR_setuid		 23
-#define __NR_getuid		 24
-#define __NR_stime		 25
-#define __NR_ptrace		 26
-#define __NR_alarm		 27
-#define __NR_oldfstat		 28
-#define __NR_pause		 29
-#define __NR_utime		 30
-#define __NR_stty		 31
-#define __NR_gtty		 32
-#define __NR_access		 33
-#define __NR_nice		 34
-#define __NR_ftime		 35
-#define __NR_sync		 36
-#define __NR_kill		 37
-#define __NR_rename		 38
-#define __NR_mkdir		 39
-#define __NR_rmdir		 40
-#define __NR_dup		 41
-#define __NR_pipe		 42
-#define __NR_times		 43
-#define __NR_prof		 44
-#define __NR_brk		 45
-#define __NR_setgid		 46
-#define __NR_getgid		 47
-#define __NR_signal		 48
-#define __NR_geteuid		 49
-#define __NR_getegid		 50
-#define __NR_acct		 51
-#define __NR_umount2		 52
-#define __NR_lock		 53
-#define __NR_ioctl		 54
-#define __NR_fcntl		 55
-#define __NR_mpx		 56
-#define __NR_setpgid		 57
-#define __NR_ulimit		 58
-#define __NR_oldolduname	 59
-#define __NR_umask		 60
-#define __NR_chroot		 61
-#define __NR_ustat		 62
-#define __NR_dup2		 63
-#define __NR_getppid		 64
-#define __NR_getpgrp		 65
-#define __NR_setsid		 66
-#define __NR_sigaction		 67
-#define __NR_sgetmask		 68
-#define __NR_ssetmask		 69
-#define __NR_setreuid		 70
-#define __NR_setregid		 71
-#define __NR_sigsuspend		 72
-#define __NR_sigpending		 73
-#define __NR_sethostname	 74
-#define __NR_setrlimit		 75
-#define __NR_getrlimit		 76	/* Back compatible 2Gig limited rlimit */
-#define __NR_getrusage		 77
-#define __NR_gettimeofday	 78
-#define __NR_settimeofday	 79
-#define __NR_getgroups		 80
-#define __NR_setgroups		 81
-#define __NR_select		 82
-#define __NR_symlink		 83
-#define __NR_oldlstat		 84
-#define __NR_readlink		 85
-#define __NR_uselib		 86
-#define __NR_swapon		 87
-#define __NR_reboot		 88
-#define __NR_readdir		 89
-#define __NR_mmap		 90
-#define __NR_munmap		 91
-#define __NR_truncate		 92
-#define __NR_ftruncate		 93
-#define __NR_fchmod		 94
-#define __NR_fchown		 95
-#define __NR_getpriority	 96
-#define __NR_setpriority	 97
-#define __NR_profil		 98
-#define __NR_statfs		 99
-#define __NR_fstatfs		100
-#define __NR_ioperm		101
-#define __NR_socketcall		102
-#define __NR_syslog		103
-#define __NR_setitimer		104
-#define __NR_getitimer		105
-#define __NR_stat		106
-#define __NR_lstat		107
-#define __NR_fstat		108
-#define __NR_olduname		109
-#define __NR_iopl		110
-#define __NR_vhangup		111
-#define __NR_idle		112
-#define __NR_vm86old		113
-#define __NR_wait4		114
-#define __NR_swapoff		115
-#define __NR_sysinfo		116
-#define __NR_ipc		117
-#define __NR_fsync		118
-#define __NR_sigreturn		119
-#define __NR_clone		120
-#define __NR_setdomainname	121
-#define __NR_uname		122
-#define __NR_modify_ldt		123
-#define __NR_adjtimex		124
-#define __NR_mprotect		125
-#define __NR_sigprocmask	126
-#define __NR_create_module	127
-#define __NR_init_module	128
-#define __NR_delete_module	129
-#define __NR_get_kernel_syms	130
-#define __NR_quotactl		131
-#define __NR_getpgid		132
-#define __NR_fchdir		133
-#define __NR_bdflush		134
-#define __NR_sysfs		135
-#define __NR_personality	136
-#define __NR_afs_syscall	137 /* Syscall for Andrew File System */
-#define __NR_setfsuid		138
-#define __NR_setfsgid		139
-#define __NR__llseek		140
-#define __NR_getdents		141
-#define __NR__newselect		142
-#define __NR_flock		143
-#define __NR_msync		144
-#define __NR_readv		145
-#define __NR_writev		146
-#define __NR_getsid		147
-#define __NR_fdatasync		148
-#define __NR__sysctl		149
-#define __NR_mlock		150
-#define __NR_munlock		151
-#define __NR_mlockall		152
-#define __NR_munlockall		153
-#define __NR_sched_setparam		154
-#define __NR_sched_getparam		155
-#define __NR_sched_setscheduler		156
-#define __NR_sched_getscheduler		157
-#define __NR_sched_yield		158
-#define __NR_sched_get_priority_max	159
-#define __NR_sched_get_priority_min	160
-#define __NR_sched_rr_get_interval	161
-#define __NR_nanosleep		162
-#define __NR_mremap		163
-#define __NR_setresuid		164
-#define __NR_getresuid		165
-#define __NR_vm86		166
-#define __NR_query_module	167
-#define __NR_poll		168
-#define __NR_nfsservctl		169
-#define __NR_setresgid		170
-#define __NR_getresgid		171
+#define __NR_exit     1
+#define __NR_fork     2
+#define __NR_read     3
+#define __NR_write      4
+#define __NR_open     5
+#define __NR_close      6
+#define __NR_waitpid      7
+#define __NR_creat      8
+#define __NR_link     9
+#define __NR_unlink    10
+#define __NR_execve    11
+#define __NR_chdir     12
+#define __NR_time    13
+#define __NR_mknod     14
+#define __NR_chmod     15
+#define __NR_lchown    16
+#define __NR_break     17
+#define __NR_oldstat     18
+#define __NR_lseek     19
+#define __NR_getpid    20
+#define __NR_mount     21
+#define __NR_umount    22
+#define __NR_setuid    23
+#define __NR_getuid    24
+#define __NR_stime     25
+#define __NR_ptrace    26
+#define __NR_alarm     27
+#define __NR_oldfstat    28
+#define __NR_pause     29
+#define __NR_utime     30
+#define __NR_stty    31
+#define __NR_gtty    32
+#define __NR_access    33
+#define __NR_nice    34
+#define __NR_ftime     35
+#define __NR_sync    36
+#define __NR_kill    37
+#define __NR_rename    38
+#define __NR_mkdir     39
+#define __NR_rmdir     40
+#define __NR_dup     41
+#define __NR_pipe    42
+#define __NR_times     43
+#define __NR_prof    44
+#define __NR_brk     45
+#define __NR_setgid    46
+#define __NR_getgid    47
+#define __NR_signal    48
+#define __NR_geteuid     49
+#define __NR_getegid     50
+#define __NR_acct    51
+#define __NR_umount2     52
+#define __NR_lock    53
+#define __NR_ioctl     54
+#define __NR_fcntl     55
+#define __NR_mpx     56
+#define __NR_setpgid     57
+#define __NR_ulimit    58
+#define __NR_oldolduname   59
+#define __NR_umask     60
+#define __NR_chroot    61
+#define __NR_ustat     62
+#define __NR_dup2    63
+#define __NR_getppid     64
+#define __NR_getpgrp     65
+#define __NR_setsid    66
+#define __NR_sigaction     67
+#define __NR_sgetmask    68
+#define __NR_ssetmask    69
+#define __NR_setreuid    70
+#define __NR_setregid    71
+#define __NR_sigsuspend    72
+#define __NR_sigpending    73
+#define __NR_sethostname   74
+#define __NR_setrlimit     75
+#define __NR_getrlimit     76 /* Back compatible 2Gig limited rlimit */
+#define __NR_getrusage     77
+#define __NR_gettimeofday  78
+#define __NR_settimeofday  79
+#define __NR_getgroups     80
+#define __NR_setgroups     81
+#define __NR_select    82
+#define __NR_symlink     83
+#define __NR_oldlstat    84
+#define __NR_readlink    85
+#define __NR_uselib    86
+#define __NR_swapon    87
+#define __NR_reboot    88
+#define __NR_readdir     89
+#define __NR_mmap    90
+#define __NR_munmap    91
+#define __NR_truncate    92
+#define __NR_ftruncate     93
+#define __NR_fchmod    94
+#define __NR_fchown    95
+#define __NR_getpriority   96
+#define __NR_setpriority   97
+#define __NR_profil    98
+#define __NR_statfs    99
+#define __NR_fstatfs    100
+#define __NR_ioperm   101
+#define __NR_socketcall   102
+#define __NR_syslog   103
+#define __NR_setitimer    104
+#define __NR_getitimer    105
+#define __NR_stat   106
+#define __NR_lstat    107
+#define __NR_fstat    108
+#define __NR_olduname   109
+#define __NR_iopl   110
+#define __NR_vhangup    111
+#define __NR_idle   112
+#define __NR_vm86old    113
+#define __NR_wait4    114
+#define __NR_swapoff    115
+#define __NR_sysinfo    116
+#define __NR_ipc    117
+#define __NR_fsync    118
+#define __NR_sigreturn    119
+#define __NR_clone    120
+#define __NR_setdomainname  121
+#define __NR_uname    122
+#define __NR_modify_ldt   123
+#define __NR_adjtimex   124
+#define __NR_mprotect   125
+#define __NR_sigprocmask  126
+#define __NR_create_module  127
+#define __NR_init_module  128
+#define __NR_delete_module  129
+#define __NR_get_kernel_syms  130
+#define __NR_quotactl   131
+#define __NR_getpgid    132
+#define __NR_fchdir   133
+#define __NR_bdflush    134
+#define __NR_sysfs    135
+#define __NR_personality  136
+#define __NR_afs_syscall  137 /* Syscall for Andrew File System */
+#define __NR_setfsuid   138
+#define __NR_setfsgid   139
+#define __NR__llseek    140
+#define __NR_getdents   141
+#define __NR__newselect   142
+#define __NR_flock    143
+#define __NR_msync    144
+#define __NR_readv    145
+#define __NR_writev   146
+#define __NR_getsid   147
+#define __NR_fdatasync    148
+#define __NR__sysctl    149
+#define __NR_mlock    150
+#define __NR_munlock    151
+#define __NR_mlockall   152
+#define __NR_munlockall   153
+#define __NR_sched_setparam   154
+#define __NR_sched_getparam   155
+#define __NR_sched_setscheduler   156
+#define __NR_sched_getscheduler   157
+#define __NR_sched_yield    158
+#define __NR_sched_get_priority_max 159
+#define __NR_sched_get_priority_min 160
+#define __NR_sched_rr_get_interval  161
+#define __NR_nanosleep    162
+#define __NR_mremap   163
+#define __NR_setresuid    164
+#define __NR_getresuid    165
+#define __NR_vm86   166
+#define __NR_query_module 167
+#define __NR_poll   168
+#define __NR_nfsservctl   169
+#define __NR_setresgid    170
+#define __NR_getresgid    171
 #define __NR_prctl              172
-#define __NR_rt_sigreturn	173
-#define __NR_rt_sigaction	174
-#define __NR_rt_sigprocmask	175
-#define __NR_rt_sigpending	176
-#define __NR_rt_sigtimedwait	177
-#define __NR_rt_sigqueueinfo	178
-#define __NR_rt_sigsuspend	179
-#define __NR_pread		180
-#define __NR_pwrite		181
-#define __NR_chown		182
-#define __NR_getcwd		183
-#define __NR_capget		184
-#define __NR_capset		185
-#define __NR_sigaltstack	186
-#define __NR_sendfile		187
-#define __NR_getpmsg		188	/* some people actually want streams */
-#define __NR_putpmsg		189	/* some people actually want streams */
-#define __NR_vfork		190
-#define __NR_ugetrlimit		191	/* SuS compliant getrlimit */
-#define __NR_mmap2		192
-#define __NR_truncate64		193
-#define __NR_ftruncate64	194
-#define __NR_stat64		195
-#define __NR_lstat64		196
-#define __NR_fstat64		197
-#define __NR_lchown32		198
-#define __NR_getuid32		199
-#define __NR_getgid32		200
-#define __NR_geteuid32		201
-#define __NR_getegid32		202
-#define __NR_setreuid32		203
-#define __NR_setregid32		204
-#define __NR_getgroups32	205
-#define __NR_setgroups32	206
-#define __NR_fchown32		207
-#define __NR_setresuid32	208
-#define __NR_getresuid32	209
-#define __NR_setresgid32	210
-#define __NR_getresgid32	211
-#define __NR_chown32		212
-#define __NR_setuid32		213
-#define __NR_setgid32		214
-#define __NR_setfsuid32		215
-#define __NR_setfsgid32		216
-#define __NR_pivot_root		217
-#define __NR_mincore		218
-#define __NR_madvise		219
-#define __NR_madvise1		219	/* delete when C lib stub is removed */
-#define __NR_getdents64		220
-#define __NR_fcntl64		221
-#define __NR_security		223	/* syscall for security modules */
-#define __NR_gettid		224
-#define __NR_readahead		225
+#define __NR_rt_sigreturn 173
+#define __NR_rt_sigaction 174
+#define __NR_rt_sigprocmask 175
+#define __NR_rt_sigpending  176
+#define __NR_rt_sigtimedwait  177
+#define __NR_rt_sigqueueinfo  178
+#define __NR_rt_sigsuspend  179
+#define __NR_pread    180
+#define __NR_pwrite   181
+#define __NR_chown    182
+#define __NR_getcwd   183
+#define __NR_capget   184
+#define __NR_capset   185
+#define __NR_sigaltstack  186
+#define __NR_sendfile   187
+#define __NR_getpmsg    188 /* some people actually want streams */
+#define __NR_putpmsg    189 /* some people actually want streams */
+#define __NR_vfork    190
+#define __NR_ugetrlimit   191 /* SuS compliant getrlimit */
+#define __NR_mmap2    192
+#define __NR_truncate64   193
+#define __NR_ftruncate64  194
+#define __NR_stat64   195
+#define __NR_lstat64    196
+#define __NR_fstat64    197
+#define __NR_lchown32   198
+#define __NR_getuid32   199
+#define __NR_getgid32   200
+#define __NR_geteuid32    201
+#define __NR_getegid32    202
+#define __NR_setreuid32   203
+#define __NR_setregid32   204
+#define __NR_getgroups32  205
+#define __NR_setgroups32  206
+#define __NR_fchown32   207
+#define __NR_setresuid32  208
+#define __NR_getresuid32  209
+#define __NR_setresgid32  210
+#define __NR_getresgid32  211
+#define __NR_chown32    212
+#define __NR_setuid32   213
+#define __NR_setgid32   214
+#define __NR_setfsuid32   215
+#define __NR_setfsgid32   216
+#define __NR_pivot_root   217
+#define __NR_mincore    218
+#define __NR_madvise    219
+#define __NR_madvise1   219 /* delete when C lib stub is removed */
+#define __NR_getdents64   220
+#define __NR_fcntl64    221
+#define __NR_rdtsc_log    222
+#define __NR_security   223 /* syscall for security modules */
+#define __NR_gettid   224
+#define __NR_readahead    225
 
 /* user-visible error numbers are in the range -1 - -124: see <asm-i386/errno.h> */
 
 #define __syscall_return(type, res) \
 do { \
-	if ((unsigned long)(res) >= (unsigned long)(-125)) { \
-		errno = -(res); \
-		res = -1; \
-	} \
-	return (type) (res); \
+  if ((unsigned long)(res) >= (unsigned long)(-125)) { \
+    errno = -(res); \
+    res = -1; \
+  } \
+  return (type) (res); \
 } while (0)
 
 /* XXX - _foo needs to be __foo, while __NR_bar could be _NR_bar. */
@@ -248,8 +249,8 @@
 { \
 long __res; \
 __asm__ volatile ("int $0x80" \
-	: "=a" (__res) \
-	: "0" (__NR_##name)); \
+  : "=a" (__res) \
+  : "0" (__NR_##name)); \
 __syscall_return(type,__res); \
 }
 
@@ -258,8 +259,8 @@
 { \
 long __res; \
 __asm__ volatile ("int $0x80" \
-	: "=a" (__res) \
-	: "0" (__NR_##name),"b" ((long)(arg1))); \
+  : "=a" (__res) \
+  : "0" (__NR_##name),"b" ((long)(arg1))); \
 __syscall_return(type,__res); \
 }
 
@@ -268,8 +269,8 @@
 { \
 long __res; \
 __asm__ volatile ("int $0x80" \
-	: "=a" (__res) \
-	: "0" (__NR_##name),"b" ((long)(arg1)),"c" ((long)(arg2))); \
+  : "=a" (__res) \
+  : "0" (__NR_##name),"b" ((long)(arg1)),"c" ((long)(arg2))); \
 __syscall_return(type,__res); \
 }
 
@@ -278,9 +279,9 @@
 { \
 long __res; \
 __asm__ volatile ("int $0x80" \
-	: "=a" (__res) \
-	: "0" (__NR_##name),"b" ((long)(arg1)),"c" ((long)(arg2)), \
-		  "d" ((long)(arg3))); \
+  : "=a" (__res) \
+  : "0" (__NR_##name),"b" ((long)(arg1)),"c" ((long)(arg2)), \
+      "d" ((long)(arg3))); \
 __syscall_return(type,__res); \
 }
 
@@ -289,34 +290,34 @@
 { \
 long __res; \
 __asm__ volatile ("int $0x80" \
-	: "=a" (__res) \
-	: "0" (__NR_##name),"b" ((long)(arg1)),"c" ((long)(arg2)), \
-	  "d" ((long)(arg3)),"S" ((long)(arg4))); \
+  : "=a" (__res) \
+  : "0" (__NR_##name),"b" ((long)(arg1)),"c" ((long)(arg2)), \
+    "d" ((long)(arg3)),"S" ((long)(arg4))); \
 __syscall_return(type,__res); \
 } 
 
 #define _syscall5(type,name,type1,arg1,type2,arg2,type3,arg3,type4,arg4, \
-	  type5,arg5) \
+    type5,arg5) \
 type name (type1 arg1,type2 arg2,type3 arg3,type4 arg4,type5 arg5) \
 { \
 long __res; \
 __asm__ volatile ("int $0x80" \
-	: "=a" (__res) \
-	: "0" (__NR_##name),"b" ((long)(arg1)),"c" ((long)(arg2)), \
-	  "d" ((long)(arg3)),"S" ((long)(arg4)),"D" ((long)(arg5))); \
+  : "=a" (__res) \
+  : "0" (__NR_##name),"b" ((long)(arg1)),"c" ((long)(arg2)), \
+    "d" ((long)(arg3)),"S" ((long)(arg4)),"D" ((long)(arg5))); \
 __syscall_return(type,__res); \
 }
 
 #define _syscall6(type,name,type1,arg1,type2,arg2,type3,arg3,type4,arg4, \
-	  type5,arg5,type6,arg6) \
+    type5,arg5,type6,arg6) \
 type name (type1 arg1,type2 arg2,type3 arg3,type4 arg4,type5 arg5,type6 arg6) \
 { \
 long __res; \
 __asm__ volatile ("push %%ebp ; movl %%eax,%%ebp ; movl %1,%%eax ; int $0x80 ; pop %%ebp" \
-	: "=a" (__res) \
-	: "i" (__NR_##name),"b" ((long)(arg1)),"c" ((long)(arg2)), \
-	  "d" ((long)(arg3)),"S" ((long)(arg4)),"D" ((long)(arg5)), \
-	  "0" ((long)(arg6))); \
+  : "=a" (__res) \
+  : "i" (__NR_##name),"b" ((long)(arg1)),"c" ((long)(arg2)), \
+    "d" ((long)(arg3)),"S" ((long)(arg4)),"D" ((long)(arg5)), \
+    "0" ((long)(arg6))); \
 __syscall_return(type,__res); \
 }
 
@@ -351,7 +352,7 @@
 
 static inline pid_t wait(int * wait_stat)
 {
-	return waitpid(-1,wait_stat,0);
+  return waitpid(-1,wait_stat,0);
 }
 
 #endif
diff -Nur -X dontdiff linux-2.4.17/include/linux/cpu_timer.h org/include/linux/cpu_timer.h
--- linux-2.4.17/include/linux/cpu_timer.h	Wed Dec 31 19:00:00 1969
+++ org/include/linux/cpu_timer.h	Thu Apr 11 01:16:19 2002
@@ -0,0 +1,87 @@
+#ifndef _LINUX_CPUTIMER_H
+#define _LINUX_CPUTIMER_H
+
+#include <asm/msr.h>
+
+
+/* new rdtsc logging functions 
+ * actula functions are declared in sched.c 
+ */
+ 
+typedef enum rdtsc_command_enum {
+    START_RDTSC,
+    READ_RDTSC,
+    STOP_RDTSC
+} rdtsc_command;
+
+typedef unsigned long       ui32_t;
+typedef unsigned long long  ui64_t;
+
+typedef struct tsc_log_s {
+    int     tsc_on;
+    ui32_t  start;
+    ui32_t  count, run_count, epoch_count;
+    ui32_t  low, high, cycles;
+    ui32_t  part[6];
+    ui32_t  total;
+    ui32_t  total_nr_running, max_nr_running;
+    ui64_t  started, stoped;
+} tsc_log_t;
+
+extern tsc_log_t tsc;
+
+static inline void start_tsc_timer(void)
+{
+    if (tsc.tsc_on) {
+        rdtscl(tsc.start);
+        tsc.count++;
+    }
+}
+
+/* This is going to be the timer for each section, array element 0 will be
+* time stamp the start point for each section */
+static inline void record_tsc_section_timer(int i)
+{
+    if (tsc.tsc_on) {
+        rdtscl(tsc.low);
+        if(i == 0)
+          tsc.part[0] = tsc.low;
+        else if ( i > 0)
+          tsc.part[i] += tsc.low - tsc.part[0];
+    }
+}
+
+static inline void record_tsc_total(void)
+{
+    if (tsc.tsc_on) {
+        rdtscl(tsc.low);
+        tsc.total += tsc.low - tsc.start;
+    }
+}
+
+
+static inline void record_tsc_calc_runqueue(int nr_running)
+{
+    if (tsc.tsc_on) {
+        tsc.total_nr_running += nr_running;
+        tsc.run_count++;
+        if (tsc.max_nr_running < nr_running)
+            tsc.max_nr_running = nr_running;
+    }
+}
+
+static inline void record_tsc_calc_epoch(void)
+{
+    if (tsc.tsc_on) {
+        tsc.epoch_count++;
+    }
+}
+
+/*
+extern void start_tsc_timer(void);
+extern void record_tsc_section_timer(int);
+extern void record_tsc_total(void);
+extern void record_tsc_calc_runqueue(int nr_running);
+*/
+
+#endif
diff -Nur -X dontdiff linux-2.4.17/kernel/cpu_timer.c org/kernel/cpu_timer.c
--- linux-2.4.17/kernel/cpu_timer.c	Wed Dec 31 19:00:00 1969
+++ org/kernel/cpu_timer.c	Thu Apr 11 01:16:23 2002
@@ -0,0 +1,44 @@
+#include <linux/slab.h>
+#include <linux/cpu_timer.h>
+#include <linux/errno.h> 
+#include <asm/uaccess.h>
+
+/* New rdtsc logging functions 
+ * Header for these functions is in "include/asm-i386/timex.h"
+ * which is included from "linux/timers.h" 
+ * Use "linux/timex.h" instead of "include/asm-i386/timex.h"
+ * when including.
+ */
+
+tsc_log_t tsc = {0};
+
+asmlinkage long sys_rdtsc_log(tsc_log_t *ret_tsc, rdtsc_command command)
+{
+    long retval = 0;
+    
+    switch (command) {
+    case START_RDTSC:
+        memset(&tsc, 0, sizeof(tsc_log_t));
+        rdtsc(tsc.low, tsc.high);
+        tsc.started = (ui64_t)tsc.high << 32 | tsc.low;
+        tsc.tsc_on = 1;
+        break;
+    case READ_RDTSC:
+        rdtsc(tsc.low, tsc.high);
+        tsc.stoped = (ui64_t)tsc.high << 32 | tsc.low;
+        copy_to_user(ret_tsc, &tsc, sizeof(tsc_log_t));
+        break;
+    case STOP_RDTSC:
+        tsc.tsc_on = 0;
+        rdtsc(tsc.low, tsc.high);
+        tsc.stoped = (ui64_t)tsc.high << 32 | tsc.low;
+        copy_to_user(ret_tsc, &tsc, sizeof(tsc_log_t));
+        break;
+    default:
+        retval = -EINVAL;
+        break;
+    }
+    
+    return retval;
+}
+
diff -Nur -X dontdiff linux-2.4.17/kernel/sched.c org/kernel/sched.c
--- linux-2.4.17/kernel/sched.c	Fri Dec 21 12:42:04 2001
+++ org/kernel/sched.c	Thu Apr 11 00:48:23 2002
@@ -7,8 +7,8 @@
  *
  *  1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and
  *              make semaphores SMP safe
- *  1998-11-19	Implemented schedule_timeout() and related stuff
- *		by Andrea Arcangeli
+ *  1998-11-19  Implemented schedule_timeout() and related stuff
+ *      by Andrea Arcangeli
  *  1998-12-28  Implemented better SMP scheduling by Ingo Molnar
  */
 
@@ -29,6 +29,8 @@
 #include <linux/completion.h>
 #include <linux/prefetch.h>
 #include <linux/compiler.h>
+#include <linux/sched.h>                    /* added by Akira */
+#include <linux/cpu_timer.h>                /* rdtsc logging functions */
 
 #include <asm/uaccess.h>
 #include <asm/mmu_context.h>
@@ -57,23 +59,23 @@
  * calculation depends on the value of HZ.
  */
 #if HZ < 200
-#define TICK_SCALE(x)	((x) >> 2)
+#define TICK_SCALE(x)   ((x) >> 2)
 #elif HZ < 400
-#define TICK_SCALE(x)	((x) >> 1)
+#define TICK_SCALE(x)   ((x) >> 1)
 #elif HZ < 800
-#define TICK_SCALE(x)	(x)
+#define TICK_SCALE(x)   (x)
 #elif HZ < 1600
-#define TICK_SCALE(x)	((x) << 1)
+#define TICK_SCALE(x)   ((x) << 1)
 #else
-#define TICK_SCALE(x)	((x) << 2)
+#define TICK_SCALE(x)   ((x) << 2)
 #endif
 
-#define NICE_TO_TICKS(nice)	(TICK_SCALE(20-(nice))+1)
+#define NICE_TO_TICKS(nice) (TICK_SCALE(20-(nice))+1)
 
 
 /*
- *	Init task must be ok at boot for the ix86 as we will check its signals
- *	via the SMP irq return path.
+ *  Init task must be ok at boot for the ix86 as we will check its signals
+ *  via the SMP irq return path.
  */
  
 struct task_struct * init_tasks[NR_CPUS] = {&init_task, };
@@ -90,7 +92,7 @@
  * task->alloc_lock nests inside tasklist_lock.
  */
 spinlock_t runqueue_lock __cacheline_aligned = SPIN_LOCK_UNLOCKED;  /* inner */
-rwlock_t tasklist_lock __cacheline_aligned = RW_LOCK_UNLOCKED;	/* outer */
+rwlock_t tasklist_lock __cacheline_aligned = RW_LOCK_UNLOCKED;  /* outer */
 
 static LIST_HEAD(runqueue_head);
 
@@ -99,11 +101,11 @@
  * to prevent cacheline ping-pong.
  */
 static union {
-	struct schedule_data {
-		struct task_struct * curr;
-		cycles_t last_schedule;
-	} schedule_data;
-	char __pad [SMP_CACHE_BYTES];
+    struct schedule_data {
+        struct task_struct * curr;
+        cycles_t last_schedule;
+    } schedule_data;
+    char __pad [SMP_CACHE_BYTES];
 } aligned_data [NR_CPUS] __cacheline_aligned = { {{&init_task,0}}};
 
 #define cpu_curr(cpu) aligned_data[(cpu)].schedule_data.curr
@@ -116,7 +118,7 @@
 
 #define idle_task(cpu) (init_tasks[cpu_number_map(cpu)])
 #define can_schedule(p,cpu) \
-	((p)->cpus_runnable & (p)->cpus_allowed & (1 << cpu))
+    ((p)->cpus_runnable & (p)->cpus_allowed & (1 << cpu))
 
 #else
 
@@ -134,63 +136,63 @@
  * and TLB miss penalties.
  *
  * Return values:
- *	 -1000: never select this
- *	     0: out of time, recalculate counters (but it might still be
- *		selected)
- *	   +ve: "goodness" value (the larger, the better)
- *	 +1000: realtime process, select this.
+ *   -1000: never select this
+ *       0: out of time, recalculate counters (but it might still be
+ *      selected)
+ *     +ve: "goodness" value (the larger, the better)
+ *   +1000: realtime process, select this.
  */
 
 static inline int goodness(struct task_struct * p, int this_cpu, struct mm_struct *this_mm)
 {
-	int weight;
+    int weight;
 
-	/*
-	 * select the current process after every other
-	 * runnable process, but before the idle thread.
-	 * Also, dont trigger a counter recalculation.
-	 */
-	weight = -1;
-	if (p->policy & SCHED_YIELD)
-		goto out;
-
-	/*
-	 * Non-RT process - normal case first.
-	 */
-	if (p->policy == SCHED_OTHER) {
-		/*
-		 * Give the process a first-approximation goodness value
-		 * according to the number of clock-ticks it has left.
-		 *
-		 * Don't do any other calculations if the time slice is
-		 * over..
-		 */
-		weight = p->counter;
-		if (!weight)
-			goto out;
-			
+    /*
+     * select the current process after every other
+     * runnable process, but before the idle thread.
+     * Also, dont trigger a counter recalculation.
+     */
+    weight = -1;
+    if (p->policy & SCHED_YIELD)
+        goto out;
+
+    /*
+     * Non-RT process - normal case first.
+     */
+    if (p->policy == SCHED_OTHER) {
+        /*
+         * Give the process a first-approximation goodness value
+         * according to the number of clock-ticks it has left.
+         *
+         * Don't do any other calculations if the time slice is
+         * over..
+         */
+        weight = p->counter;
+        if (!weight)
+            goto out;
+            
 #ifdef CONFIG_SMP
-		/* Give a largish advantage to the same processor...   */
-		/* (this is equivalent to penalizing other processors) */
-		if (p->processor == this_cpu)
-			weight += PROC_CHANGE_PENALTY;
+        /* Give a largish advantage to the same processor...   */
+        /* (this is equivalent to penalizing other processors) */
+        if (p->processor == this_cpu)
+            weight += PROC_CHANGE_PENALTY;
 #endif
 
-		/* .. and a slight advantage to the current MM */
-		if (p->mm == this_mm || !p->mm)
-			weight += 1;
-		weight += 20 - p->nice;
-		goto out;
-	}
-
-	/*
-	 * Realtime process, select the first one on the
-	 * runqueue (taking priorities within processes
-	 * into account).
-	 */
-	weight = 1000 + p->rt_priority;
+        /* .. and a slight advantage to the current MM */
+        if (p->mm == this_mm || !p->mm)
+            weight += 1;
+        weight += 20 - p->nice;
+        goto out;
+    }
+
+    /*
+     * Realtime process, select the first one on the
+     * runqueue (taking priorities within processes
+     * into account).
+     */
+    weight = 1000 + p->rt_priority;
 out:
-	return weight;
+    return weight;
 }
 
 /*
@@ -199,7 +201,7 @@
  */
 static inline int preemption_goodness(struct task_struct * prev, struct task_struct * p, int cpu)
 {
-	return goodness(p, cpu, prev->active_mm) - goodness(prev, cpu, prev->active_mm);
+    return goodness(p, cpu, prev->active_mm) - goodness(prev, cpu, prev->active_mm);
 }
 
 /*
@@ -212,106 +214,106 @@
 static void reschedule_idle(struct task_struct * p)
 {
 #ifdef CONFIG_SMP
-	int this_cpu = smp_processor_id();
-	struct task_struct *tsk, *target_tsk;
-	int cpu, best_cpu, i, max_prio;
-	cycles_t oldest_idle;
-
-	/*
-	 * shortcut if the woken up task's last CPU is
-	 * idle now.
-	 */
-	best_cpu = p->processor;
-	if (can_schedule(p, best_cpu)) {
-		tsk = idle_task(best_cpu);
-		if (cpu_curr(best_cpu) == tsk) {
-			int need_resched;
+    int this_cpu = smp_processor_id();
+    struct task_struct *tsk, *target_tsk;
+    int cpu, best_cpu, i, max_prio;
+    cycles_t oldest_idle;
+
+    /*
+     * shortcut if the woken up task's last CPU is
+     * idle now.
+     */
+    best_cpu = p->processor;
+    if (can_schedule(p, best_cpu)) {
+        tsk = idle_task(best_cpu);
+        if (cpu_curr(best_cpu) == tsk) {
+            int need_resched;
 send_now_idle:
-			/*
-			 * If need_resched == -1 then we can skip sending
-			 * the IPI altogether, tsk->need_resched is
-			 * actively watched by the idle thread.
-			 */
-			need_resched = tsk->need_resched;
-			tsk->need_resched = 1;
-			if ((best_cpu != this_cpu) && !need_resched)
-				smp_send_reschedule(best_cpu);
-			return;
-		}
-	}
-
-	/*
-	 * We know that the preferred CPU has a cache-affine current
-	 * process, lets try to find a new idle CPU for the woken-up
-	 * process. Select the least recently active idle CPU. (that
-	 * one will have the least active cache context.) Also find
-	 * the executing process which has the least priority.
-	 */
-	oldest_idle = (cycles_t) -1;
-	target_tsk = NULL;
-	max_prio = 0;
-
-	for (i = 0; i < smp_num_cpus; i++) {
-		cpu = cpu_logical_map(i);
-		if (!can_schedule(p, cpu))
-			continue;
-		tsk = cpu_curr(cpu);
-		/*
-		 * We use the first available idle CPU. This creates
-		 * a priority list between idle CPUs, but this is not
-		 * a problem.
-		 */
-		if (tsk == idle_task(cpu)) {
+            /*
+             * If need_resched == -1 then we can skip sending
+             * the IPI altogether, tsk->need_resched is
+             * actively watched by the idle thread.
+             */
+            need_resched = tsk->need_resched;
+            tsk->need_resched = 1;
+            if ((best_cpu != this_cpu) && !need_resched)
+                smp_send_reschedule(best_cpu);
+            return;
+        }
+    }
+
+    /*
+     * We know that the preferred CPU has a cache-affine current
+     * process, lets try to find a new idle CPU for the woken-up
+     * process. Select the least recently active idle CPU. (that
+     * one will have the least active cache context.) Also find
+     * the executing process which has the least priority.
+     */
+    oldest_idle = (cycles_t) -1;
+    target_tsk = NULL;
+    max_prio = 0;
+
+    for (i = 0; i < smp_num_cpus; i++) {
+        cpu = cpu_logical_map(i);
+        if (!can_schedule(p, cpu))
+            continue;
+        tsk = cpu_curr(cpu);
+        /*
+         * We use the first available idle CPU. This creates
+         * a priority list between idle CPUs, but this is not
+         * a problem.
+         */
+        if (tsk == idle_task(cpu)) {
 #if defined(__i386__) && defined(CONFIG_SMP)
                         /*
-			 * Check if two siblings are idle in the same
-			 * physical package. Use them if found.
-			 */
-			if (smp_num_siblings == 2) {
-				if (cpu_curr(cpu_sibling_map[cpu]) == 
-			            idle_task(cpu_sibling_map[cpu])) {
-					oldest_idle = last_schedule(cpu);
-					target_tsk = tsk;
-					break;
-				}
-				
+             * Check if two siblings are idle in the same
+             * physical package. Use them if found.
+             */
+            if (smp_num_siblings == 2) {
+                if (cpu_curr(cpu_sibling_map[cpu]) == 
+                        idle_task(cpu_sibling_map[cpu])) {
+                    oldest_idle = last_schedule(cpu);
+                    target_tsk = tsk;
+                    break;
+                }
+                
                         }
-#endif		
-			if (last_schedule(cpu) < oldest_idle) {
-				oldest_idle = last_schedule(cpu);
-				target_tsk = tsk;
-			}
-		} else {
-			if (oldest_idle == -1ULL) {
-				int prio = preemption_goodness(tsk, p, cpu);
-
-				if (prio > max_prio) {
-					max_prio = prio;
-					target_tsk = tsk;
-				}
-			}
-		}
-	}
-	tsk = target_tsk;
-	if (tsk) {
-		if (oldest_idle != -1ULL) {
-			best_cpu = tsk->processor;
-			goto send_now_idle;
-		}
-		tsk->need_resched = 1;
-		if (tsk->processor != this_cpu)
-			smp_send_reschedule(tsk->processor);
-	}
-	return;
-		
+#endif      
+            if (last_schedule(cpu) < oldest_idle) {
+                oldest_idle = last_schedule(cpu);
+                target_tsk = tsk;
+            }
+        } else {
+            if (oldest_idle == -1ULL) {
+                int prio = preemption_goodness(tsk, p, cpu);
+
+                if (prio > max_prio) {
+                    max_prio = prio;
+                    target_tsk = tsk;
+                }
+            }
+        }
+    }
+    tsk = target_tsk;
+    if (tsk) {
+        if (oldest_idle != -1ULL) {
+            best_cpu = tsk->processor;
+            goto send_now_idle;
+        }
+        tsk->need_resched = 1;
+        if (tsk->processor != this_cpu)
+            smp_send_reschedule(tsk->processor);
+    }
+    return;
+        
 
 #else /* UP */
-	int this_cpu = smp_processor_id();
-	struct task_struct *tsk;
+    int this_cpu = smp_processor_id();
+    struct task_struct *tsk;
 
-	tsk = cpu_curr(this_cpu);
-	if (preemption_goodness(tsk, p, this_cpu) > 0)
-		tsk->need_resched = 1;
+    tsk = cpu_curr(this_cpu);
+    if (preemption_goodness(tsk, p, this_cpu) > 0)
+        tsk->need_resched = 1;
 #endif
 }
 
@@ -324,20 +326,20 @@
  */
 static inline void add_to_runqueue(struct task_struct * p)
 {
-	list_add(&p->run_list, &runqueue_head);
-	nr_running++;
+    list_add(&p->run_list, &runqueue_head);
+    nr_running++;
 }
 
 static inline void move_last_runqueue(struct task_struct * p)
 {
-	list_del(&p->run_list);
-	list_add_tail(&p->run_list, &runqueue_head);
+    list_del(&p->run_list);
+    list_add_tail(&p->run_list, &runqueue_head);
 }
 
 static inline void move_first_runqueue(struct task_struct * p)
 {
-	list_del(&p->run_list);
-	list_add(&p->run_list, &runqueue_head);
+    list_del(&p->run_list);
+    list_add(&p->run_list, &runqueue_head);
 }
 
 /*
@@ -350,35 +352,35 @@
  */
 static inline int try_to_wake_up(struct task_struct * p, int synchronous)
 {
-	unsigned long flags;
-	int success = 0;
+    unsigned long flags;
+    int success = 0;
 
-	/*
-	 * We want the common case fall through straight, thus the goto.
-	 */
-	spin_lock_irqsave(&runqueue_lock, flags);
-	p->state = TASK_RUNNING;
-	if (task_on_runqueue(p))
-		goto out;
-	add_to_runqueue(p);
-	if (!synchronous || !(p->cpus_allowed & (1 << smp_processor_id())))
-		reschedule_idle(p);
-	success = 1;
+    /*
+     * We want the common case fall through straight, thus the goto.
+     */
+    spin_lock_irqsave(&runqueue_lock, flags);
+    p->state = TASK_RUNNING;
+    if (task_on_runqueue(p))
+        goto out;
+    add_to_runqueue(p);
+    if (!synchronous || !(p->cpus_allowed & (1 << smp_processor_id())))
+        reschedule_idle(p);
+    success = 1;
 out:
-	spin_unlock_irqrestore(&runqueue_lock, flags);
-	return success;
+    spin_unlock_irqrestore(&runqueue_lock, flags);
+    return success;
 }
 
 inline int wake_up_process(struct task_struct * p)
 {
-	return try_to_wake_up(p, 0);
+    return try_to_wake_up(p, 0);
 }
 
 static void process_timeout(unsigned long __data)
 {
-	struct task_struct * p = (struct task_struct *) __data;
+    struct task_struct * p = (struct task_struct *) __data;
 
-	wake_up_process(p);
+    wake_up_process(p);
 }
 
 /**
@@ -409,54 +411,54 @@
  */
 signed long schedule_timeout(signed long timeout)
 {
-	struct timer_list timer;
-	unsigned long expire;
+    struct timer_list timer;
+    unsigned long expire;
 
-	switch (timeout)
-	{
-	case MAX_SCHEDULE_TIMEOUT:
-		/*
-		 * These two special cases are useful to be comfortable
-		 * in the caller. Nothing more. We could take
-		 * MAX_SCHEDULE_TIMEOUT from one of the negative value
-		 * but I' d like to return a valid offset (>=0) to allow
-		 * the caller to do everything it want with the retval.
-		 */
-		schedule();
-		goto out;
-	default:
-		/*
-		 * Another bit of PARANOID. Note that the retval will be
-		 * 0 since no piece of kernel is supposed to do a check
-		 * for a negative retval of schedule_timeout() (since it
-		 * should never happens anyway). You just have the printk()
-		 * that will tell you if something is gone wrong and where.
-		 */
-		if (timeout < 0)
-		{
-			printk(KERN_ERR "schedule_timeout: wrong timeout "
-			       "value %lx from %p\n", timeout,
-			       __builtin_return_address(0));
-			current->state = TASK_RUNNING;
-			goto out;
-		}
-	}
-
-	expire = timeout + jiffies;
-
-	init_timer(&timer);
-	timer.expires = expire;
-	timer.data = (unsigned long) current;
-	timer.function = process_timeout;
-
-	add_timer(&timer);
-	schedule();
-	del_timer_sync(&timer);
+    switch (timeout)
+    {
+    case MAX_SCHEDULE_TIMEOUT:
+        /*
+         * These two special cases are useful to be comfortable
+         * in the caller. Nothing more. We could take
+         * MAX_SCHEDULE_TIMEOUT from one of the negative value
+         * but I' d like to return a valid offset (>=0) to allow
+         * the caller to do everything it want with the retval.
+         */
+        schedule();
+        goto out;
+    default:
+        /*
+         * Another bit of PARANOID. Note that the retval will be
+         * 0 since no piece of kernel is supposed to do a check
+         * for a negative retval of schedule_timeout() (since it
+         * should never happens anyway). You just have the printk()
+         * that will tell you if something is gone wrong and where.
+         */
+        if (timeout < 0)
+        {
+            printk(KERN_ERR "schedule_timeout: wrong timeout "
+                   "value %lx from %p\n", timeout,
+                   __builtin_return_address(0));
+            current->state = TASK_RUNNING;
+            goto out;
+        }
+    }
+
+    expire = timeout + jiffies;
+
+    init_timer(&timer);
+    timer.expires = expire;
+    timer.data = (unsigned long) current;
+    timer.function = process_timeout;
+
+    add_timer(&timer);
+    schedule();
+    del_timer_sync(&timer);
 
-	timeout = expire - jiffies;
+    timeout = expire - jiffies;
 
  out:
-	return timeout < 0 ? 0 : timeout;
+    return timeout < 0 ? 0 : timeout;
 }
 
 /*
@@ -467,73 +469,73 @@
 static inline void __schedule_tail(struct task_struct *prev)
 {
 #ifdef CONFIG_SMP
-	int policy;
+    int policy;
 
-	/*
-	 * prev->policy can be written from here only before `prev'
-	 * can be scheduled (before setting prev->cpus_runnable to ~0UL).
-	 * Of course it must also be read before allowing prev
-	 * to be rescheduled, but since the write depends on the read
-	 * to complete, wmb() is enough. (the spin_lock() acquired
-	 * before setting cpus_runnable is not enough because the spin_lock()
-	 * common code semantics allows code outside the critical section
-	 * to enter inside the critical section)
-	 */
-	policy = prev->policy;
-	prev->policy = policy & ~SCHED_YIELD;
-	wmb();
-
-	/*
-	 * fast path falls through. We have to clear cpus_runnable before
-	 * checking prev->state to avoid a wakeup race. Protect against
-	 * the task exiting early.
-	 */
-	task_lock(prev);
-	task_release_cpu(prev);
-	mb();
-	if (prev->state == TASK_RUNNING)
-		goto needs_resched;
+    /*
+     * prev->policy can be written from here only before `prev'
+     * can be scheduled (before setting prev->cpus_runnable to ~0UL).
+     * Of course it must also be read before allowing prev
+     * to be rescheduled, but since the write depends on the read
+     * to complete, wmb() is enough. (the spin_lock() acquired
+     * before setting cpus_runnable is not enough because the spin_lock()
+     * common code semantics allows code outside the critical section
+     * to enter inside the critical section)
+     */
+    policy = prev->policy;
+    prev->policy = policy & ~SCHED_YIELD;
+    wmb();
+
+    /*
+     * fast path falls through. We have to clear cpus_runnable before
+     * checking prev->state to avoid a wakeup race. Protect against
+     * the task exiting early.
+     */
+    task_lock(prev);
+    task_release_cpu(prev);
+    mb();
+    if (prev->state == TASK_RUNNING)
+        goto needs_resched;
 
 out_unlock:
-	task_unlock(prev);	/* Synchronise here with release_task() if prev is TASK_ZOMBIE */
-	return;
+    task_unlock(prev);  /* Synchronise here with release_task() if prev is TASK_ZOMBIE */
+    return;
 
-	/*
-	 * Slow path - we 'push' the previous process and
-	 * reschedule_idle() will attempt to find a new
-	 * processor for it. (but it might preempt the
-	 * current process as well.) We must take the runqueue
-	 * lock and re-check prev->state to be correct. It might
-	 * still happen that this process has a preemption
-	 * 'in progress' already - but this is not a problem and
-	 * might happen in other circumstances as well.
-	 */
+    /*
+     * Slow path - we 'push' the previous process and
+     * reschedule_idle() will attempt to find a new
+     * processor for it. (but it might preempt the
+     * current process as well.) We must take the runqueue
+     * lock and re-check prev->state to be correct. It might
+     * still happen that this process has a preemption
+     * 'in progress' already - but this is not a problem and
+     * might happen in other circumstances as well.
+     */
 needs_resched:
-	{
-		unsigned long flags;
+    {
+        unsigned long flags;
 
-		/*
-		 * Avoid taking the runqueue lock in cases where
-		 * no preemption-check is necessery:
-		 */
-		if ((prev == idle_task(smp_processor_id())) ||
-						(policy & SCHED_YIELD))
-			goto out_unlock;
-
-		spin_lock_irqsave(&runqueue_lock, flags);
-		if ((prev->state == TASK_RUNNING) && !task_has_cpu(prev))
-			reschedule_idle(prev);
-		spin_unlock_irqrestore(&runqueue_lock, flags);
-		goto out_unlock;
-	}
+        /*
+         * Avoid taking the runqueue lock in cases where
+         * no preemption-check is necessery:
+         */
+        if ((prev == idle_task(smp_processor_id())) ||
+                        (policy & SCHED_YIELD))
+            goto out_unlock;
+
+        spin_lock_irqsave(&runqueue_lock, flags);
+        if ((prev->state == TASK_RUNNING) && !task_has_cpu(prev))
+            reschedule_idle(prev);
+        spin_unlock_irqrestore(&runqueue_lock, flags);
+        goto out_unlock;
+    }
 #else
-	prev->policy &= ~SCHED_YIELD;
+    prev->policy &= ~SCHED_YIELD;
 #endif /* CONFIG_SMP */
 }
 
 asmlinkage void schedule_tail(struct task_struct *prev)
 {
-	__schedule_tail(prev);
+    __schedule_tail(prev);
 }
 
 /*
@@ -546,162 +548,185 @@
  * tasks can run. It can not be killed, and it cannot sleep. The 'state'
  * information in task[0] is never used.
  */
+
 asmlinkage void schedule(void)
 {
-	struct schedule_data * sched_data;
-	struct task_struct *prev, *next, *p;
-	struct list_head *tmp;
-	int this_cpu, c;
+    struct schedule_data * sched_data;
+    struct task_struct *prev, *next, *p;
+    struct list_head *tmp;
+    int this_cpu, c;
 
+    /*logging starts here!*/
+    start_tsc_timer();      /* start log ********************************/
 
-	spin_lock_prefetch(&runqueue_lock);
+    spin_lock_prefetch(&runqueue_lock);
 
-	if (!current->active_mm) BUG();
+    if (!current->active_mm) BUG();
 need_resched_back:
-	prev = current;
-	this_cpu = prev->processor;
+    prev = current;
+    this_cpu = prev->processor;
 
-	if (unlikely(in_interrupt())) {
-		printk("Scheduling in interrupt\n");
-		BUG();
-	}
-
-	release_kernel_lock(prev, this_cpu);
-
-	/*
-	 * 'sched_data' is protected by the fact that we can run
-	 * only one process per CPU.
-	 */
-	sched_data = & aligned_data[this_cpu].schedule_data;
-
-	spin_lock_irq(&runqueue_lock);
-
-	/* move an exhausted RR process to be last.. */
-	if (unlikely(prev->policy == SCHED_RR))
-		if (!prev->counter) {
-			prev->counter = NICE_TO_TICKS(prev->nice);
-			move_last_runqueue(prev);
-		}
-
-	switch (prev->state) {
-		case TASK_INTERRUPTIBLE:
-			if (signal_pending(prev)) {
-				prev->state = TASK_RUNNING;
-				break;
-			}
-		default:
-			del_from_runqueue(prev);
-		case TASK_RUNNING:;
-	}
-	prev->need_resched = 0;
-
-	/*
-	 * this is the scheduler proper:
-	 */
+    if (unlikely(in_interrupt())) {
+        printk("Scheduling in interrupt\n");
+        BUG();
+    }
+
+    release_kernel_lock(prev, this_cpu);
+
+    /*
+     * 'sched_data' is protected by the fact that we can run
+     * only one process per CPU.
+     */
+    sched_data = & aligned_data[this_cpu].schedule_data;
+
+    spin_lock_irq(&runqueue_lock);
+
+    /* move an exhausted RR process to be last.. */
+    if (unlikely(prev->policy == SCHED_RR))
+        if (!prev->counter) {
+            prev->counter = NICE_TO_TICKS(prev->nice);
+            move_last_runqueue(prev);
+        }
+
+    switch (prev->state) {
+        case TASK_INTERRUPTIBLE:
+            if (signal_pending(prev)) {
+                prev->state = TASK_RUNNING;
+                break;
+            }
+        default:
+            del_from_runqueue(prev);
+        case TASK_RUNNING:;
+    }
+    prev->need_resched = 0;
+
+    /*
+     * this is the scheduler proper:
+     */
 
 repeat_schedule:
-	/*
-	 * Default process to select..
-	 */
-	next = idle_task(this_cpu);
-	c = -1000;
-	list_for_each(tmp, &runqueue_head) {
-		p = list_entry(tmp, struct task_struct, run_list);
-		if (can_schedule(p, this_cpu)) {
-			int weight = goodness(p, this_cpu, prev->active_mm);
-			if (weight > c)
-				c = weight, next = p;
-		}
-	}
-
-	/* Do we need to re-calculate counters? */
-	if (unlikely(!c)) {
-		struct task_struct *p;
-
-		spin_unlock_irq(&runqueue_lock);
-		read_lock(&tasklist_lock);
-		for_each_task(p)
-			p->counter = (p->counter >> 1) + NICE_TO_TICKS(p->nice);
-		read_unlock(&tasklist_lock);
-		spin_lock_irq(&runqueue_lock);
-		goto repeat_schedule;
-	}
-
-	/*
-	 * from this point on nothing can prevent us from
-	 * switching to the next task, save this fact in
-	 * sched_data.
-	 */
-	sched_data->curr = next;
-	task_set_cpu(next, this_cpu);
-	spin_unlock_irq(&runqueue_lock);
-
-	if (unlikely(prev == next)) {
-		/* We won't go through the normal tail, so do this by hand */
-		prev->policy &= ~SCHED_YIELD;
-		goto same_process;
-	}
+    /*
+     * Default process to select..
+     */
+    record_tsc_section_timer(0); /* sec 0 log ********************************/
+    /* O(N) scheduler at its best, benchmarking performance 
+     * on every selection */
+    record_tsc_calc_runqueue(nr_running); /* record tasks in the runqueue*/
+    
+    next = idle_task(this_cpu);
+    c = -1000;
+    list_for_each(tmp, &runqueue_head) {
+        p = list_entry(tmp, struct task_struct, run_list);
+        if (can_schedule(p, this_cpu)) {
+            int weight = goodness(p, this_cpu, prev->active_mm);
+            if (weight > c)
+                c = weight, next = p;
+        }
+    }
+
+    record_tsc_section_timer(1); /* sec 1 log ********************************/
+
+    
+    /* Do we need to re-calculate counters? */
+    if (unlikely(!c)) {
+        struct task_struct *p;
+        record_tsc_section_timer(0); /* sec 0 log ****************************/
+        /* Calculating the amount of time consumed for recalculation of 
+         * run queue values*/
+        record_tsc_calc_epoch(); /* calc epoch *******************************/
+        
+        spin_unlock_irq(&runqueue_lock);
+        read_lock(&tasklist_lock);
+        for_each_task(p)
+            p->counter = (p->counter >> 1) + NICE_TO_TICKS(p->nice);
+        read_unlock(&tasklist_lock);
+        spin_lock_irq(&runqueue_lock);
+    
+        record_tsc_section_timer(2); /* sec 2 log ****************************/     
+        
+    goto repeat_schedule;
+    }
+
+    record_tsc_section_timer(0); /* sec 0 log ********************************/
+    /*
+     * from this point on nothing can prevent us from
+     * switching to the next task, save this fact in
+     * sched_data.
+     */
+    sched_data->curr = next;
+    task_set_cpu(next, this_cpu);
+    spin_unlock_irq(&runqueue_lock);
+
+    if (unlikely(prev == next)) {
+        /* We won't go through the normal tail, so do this by hand */
+        prev->policy &= ~SCHED_YIELD;
+        goto same_process;
+    }
 
 #ifdef CONFIG_SMP
- 	/*
- 	 * maintain the per-process 'last schedule' value.
- 	 * (this has to be recalculated even if we reschedule to
- 	 * the same process) Currently this is only used on SMP,
-	 * and it's approximate, so we do not have to maintain
-	 * it while holding the runqueue spinlock.
- 	 */
- 	sched_data->last_schedule = get_cycles();
-
-	/*
-	 * We drop the scheduler lock early (it's a global spinlock),
-	 * thus we have to lock the previous process from getting
-	 * rescheduled during switch_to().
-	 */
+    /*
+     * maintain the per-process 'last schedule' value.
+     * (this has to be recalculated even if we reschedule to
+     * the same process) Currently this is only used on SMP,
+     * and it's approximate, so we do not have to maintain
+     * it while holding the runqueue spinlock.
+     */
+    sched_data->last_schedule = get_cycles();
+
+    /*
+     * We drop the scheduler lock early (it's a global spinlock),
+     * thus we have to lock the previous process from getting
+     * rescheduled during switch_to().
+     */
 
 #endif /* CONFIG_SMP */
 
-	kstat.context_swtch++;
-	/*
-	 * there are 3 processes which are affected by a context switch:
-	 *
-	 * prev == .... ==> (last => next)
-	 *
-	 * It's the 'much more previous' 'prev' that is on next's stack,
-	 * but prev is set to (the just run) 'last' process by switch_to().
-	 * This might sound slightly confusing but makes tons of sense.
-	 */
-	prepare_to_switch();
-	{
-		struct mm_struct *mm = next->mm;
-		struct mm_struct *oldmm = prev->active_mm;
-		if (!mm) {
-			if (next->active_mm) BUG();
-			next->active_mm = oldmm;
-			atomic_inc(&oldmm->mm_count);
-			enter_lazy_tlb(oldmm, next, this_cpu);
-		} else {
-			if (next->active_mm != mm) BUG();
-			switch_mm(oldmm, mm, next, this_cpu);
-		}
-
-		if (!prev->mm) {
-			prev->active_mm = NULL;
-			mmdrop(oldmm);
-		}
-	}
-
-	/*
-	 * This just switches the register state and the
-	 * stack.
-	 */
-	switch_to(prev, next, prev);
-	__schedule_tail(prev);
+    kstat.context_swtch++;
+    /*
+     * there are 3 processes which are affected by a context switch:
+     *
+     * prev == .... ==> (last => next)
+     *
+     * It's the 'much more previous' 'prev' that is on next's stack,
+     * but prev is set to (the just run) 'last' process by switch_to().
+     * This might sound slightly confusing but makes tons of sense.
+     */
+    prepare_to_switch();
+    {
+        struct mm_struct *mm = next->mm;
+        struct mm_struct *oldmm = prev->active_mm;
+        if (!mm) {
+            if (next->active_mm) BUG();
+            next->active_mm = oldmm;
+            atomic_inc(&oldmm->mm_count);
+            enter_lazy_tlb(oldmm, next, this_cpu);
+        } else {
+            if (next->active_mm != mm) BUG();
+            switch_mm(oldmm, mm, next, this_cpu);
+        }
+
+        if (!prev->mm) {
+            prev->active_mm = NULL;
+            mmdrop(oldmm);
+        }
+    }
+
+    /*
+     * This just switches the register state and the
+     * stack.
+     */
+    switch_to(prev, next, prev);
+    __schedule_tail(prev);
 
 same_process:
-	reacquire_kernel_lock(current);
-	if (current->need_resched)
-		goto need_resched_back;
-	return;
+    reacquire_kernel_lock(current);
+    if (current->need_resched)
+        goto need_resched_back;
+
+    record_tsc_section_timer(3); /* sec 3 log ********************************/
+    record_tsc_total();     /* total   ***************************************/
+
+    return;
 }
 
 /*
@@ -714,140 +739,140 @@
  * in this (rare) case, and we handle it by contonuing to scan the queue.
  */
 static inline void __wake_up_common (wait_queue_head_t *q, unsigned int mode,
-			 	     int nr_exclusive, const int sync)
+                     int nr_exclusive, const int sync)
 {
-	struct list_head *tmp;
-	struct task_struct *p;
+    struct list_head *tmp;
+    struct task_struct *p;
 
-	CHECK_MAGIC_WQHEAD(q);
-	WQ_CHECK_LIST_HEAD(&q->task_list);
-	
-	list_for_each(tmp,&q->task_list) {
-		unsigned int state;
+    CHECK_MAGIC_WQHEAD(q);
+    WQ_CHECK_LIST_HEAD(&q->task_list);
+    
+    list_for_each(tmp,&q->task_list) {
+        unsigned int state;
                 wait_queue_t *curr = list_entry(tmp, wait_queue_t, task_list);
 
-		CHECK_MAGIC(curr->__magic);
-		p = curr->task;
-		state = p->state;
-		if (state & mode) {
-			WQ_NOTE_WAKER(curr);
-			if (try_to_wake_up(p, sync) && (curr->flags&WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
-				break;
-		}
-	}
+        CHECK_MAGIC(curr->__magic);
+        p = curr->task;
+        state = p->state;
+        if (state & mode) {
+            WQ_NOTE_WAKER(curr);
+            if (try_to_wake_up(p, sync) && (curr->flags&WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
+                break;
+        }
+    }
 }
 
 void __wake_up(wait_queue_head_t *q, unsigned int mode, int nr)
 {
-	if (q) {
-		unsigned long flags;
-		wq_read_lock_irqsave(&q->lock, flags);
-		__wake_up_common(q, mode, nr, 0);
-		wq_read_unlock_irqrestore(&q->lock, flags);
-	}
+    if (q) {
+        unsigned long flags;
+        wq_read_lock_irqsave(&q->lock, flags);
+        __wake_up_common(q, mode, nr, 0);
+        wq_read_unlock_irqrestore(&q->lock, flags);
+    }
 }
 
 void __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr)
 {
-	if (q) {
-		unsigned long flags;
-		wq_read_lock_irqsave(&q->lock, flags);
-		__wake_up_common(q, mode, nr, 1);
-		wq_read_unlock_irqrestore(&q->lock, flags);
-	}
+    if (q) {
+        unsigned long flags;
+        wq_read_lock_irqsave(&q->lock, flags);
+        __wake_up_common(q, mode, nr, 1);
+        wq_read_unlock_irqrestore(&q->lock, flags);
+    }
 }
 
 void complete(struct completion *x)
 {
-	unsigned long flags;
+    unsigned long flags;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
-	x->done++;
-	__wake_up_common(&x->wait, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, 1, 0);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+    spin_lock_irqsave(&x->wait.lock, flags);
+    x->done++;
+    __wake_up_common(&x->wait, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, 1, 0);
+    spin_unlock_irqrestore(&x->wait.lock, flags);
 }
 
 void wait_for_completion(struct completion *x)
 {
-	spin_lock_irq(&x->wait.lock);
-	if (!x->done) {
-		DECLARE_WAITQUEUE(wait, current);
-
-		wait.flags |= WQ_FLAG_EXCLUSIVE;
-		__add_wait_queue_tail(&x->wait, &wait);
-		do {
-			__set_current_state(TASK_UNINTERRUPTIBLE);
-			spin_unlock_irq(&x->wait.lock);
-			schedule();
-			spin_lock_irq(&x->wait.lock);
-		} while (!x->done);
-		__remove_wait_queue(&x->wait, &wait);
-	}
-	x->done--;
-	spin_unlock_irq(&x->wait.lock);
-}
-
-#define	SLEEP_ON_VAR				\
-	unsigned long flags;			\
-	wait_queue_t wait;			\
-	init_waitqueue_entry(&wait, current);
-
-#define	SLEEP_ON_HEAD					\
-	wq_write_lock_irqsave(&q->lock,flags);		\
-	__add_wait_queue(q, &wait);			\
-	wq_write_unlock(&q->lock);
-
-#define	SLEEP_ON_TAIL						\
-	wq_write_lock_irq(&q->lock);				\
-	__remove_wait_queue(q, &wait);				\
-	wq_write_unlock_irqrestore(&q->lock,flags);
+    spin_lock_irq(&x->wait.lock);
+    if (!x->done) {
+        DECLARE_WAITQUEUE(wait, current);
+
+        wait.flags |= WQ_FLAG_EXCLUSIVE;
+        __add_wait_queue_tail(&x->wait, &wait);
+        do {
+            __set_current_state(TASK_UNINTERRUPTIBLE);
+            spin_unlock_irq(&x->wait.lock);
+            schedule();
+            spin_lock_irq(&x->wait.lock);
+        } while (!x->done);
+        __remove_wait_queue(&x->wait, &wait);
+    }
+    x->done--;
+    spin_unlock_irq(&x->wait.lock);
+}
+
+#define SLEEP_ON_VAR                \
+    unsigned long flags;            \
+    wait_queue_t wait;          \
+    init_waitqueue_entry(&wait, current);
+
+#define SLEEP_ON_HEAD                   \
+    wq_write_lock_irqsave(&q->lock,flags);      \
+    __add_wait_queue(q, &wait);         \
+    wq_write_unlock(&q->lock);
+
+#define SLEEP_ON_TAIL                       \
+    wq_write_lock_irq(&q->lock);                \
+    __remove_wait_queue(q, &wait);              \
+    wq_write_unlock_irqrestore(&q->lock,flags);
 
 void interruptible_sleep_on(wait_queue_head_t *q)
 {
-	SLEEP_ON_VAR
+    SLEEP_ON_VAR
 
-	current->state = TASK_INTERRUPTIBLE;
+    current->state = TASK_INTERRUPTIBLE;
 
-	SLEEP_ON_HEAD
-	schedule();
-	SLEEP_ON_TAIL
+    SLEEP_ON_HEAD
+    schedule();
+    SLEEP_ON_TAIL
 }
 
 long interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
-	SLEEP_ON_VAR
+    SLEEP_ON_VAR
 
-	current->state = TASK_INTERRUPTIBLE;
+    current->state = TASK_INTERRUPTIBLE;
 
-	SLEEP_ON_HEAD
-	timeout = schedule_timeout(timeout);
-	SLEEP_ON_TAIL
+    SLEEP_ON_HEAD
+    timeout = schedule_timeout(timeout);
+    SLEEP_ON_TAIL
 
-	return timeout;
+    return timeout;
 }
 
 void sleep_on(wait_queue_head_t *q)
 {
-	SLEEP_ON_VAR
-	
-	current->state = TASK_UNINTERRUPTIBLE;
-
-	SLEEP_ON_HEAD
-	schedule();
-	SLEEP_ON_TAIL
+    SLEEP_ON_VAR
+    
+    current->state = TASK_UNINTERRUPTIBLE;
+
+    SLEEP_ON_HEAD
+    schedule();
+    SLEEP_ON_TAIL
 }
 
 long sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
-	SLEEP_ON_VAR
-	
-	current->state = TASK_UNINTERRUPTIBLE;
-
-	SLEEP_ON_HEAD
-	timeout = schedule_timeout(timeout);
-	SLEEP_ON_TAIL
+    SLEEP_ON_VAR
+    
+    current->state = TASK_UNINTERRUPTIBLE;
+
+    SLEEP_ON_HEAD
+    timeout = schedule_timeout(timeout);
+    SLEEP_ON_TAIL
 
-	return timeout;
+    return timeout;
 }
 
 void scheduling_functions_end_here(void) { }
@@ -862,359 +887,359 @@
 
 asmlinkage long sys_nice(int increment)
 {
-	long newprio;
+    long newprio;
 
-	/*
-	 *	Setpriority might change our priority at the same moment.
-	 *	We don't have to worry. Conceptually one call occurs first
-	 *	and we have a single winner.
-	 */
-	if (increment < 0) {
-		if (!capable(CAP_SYS_NICE))
-			return -EPERM;
-		if (increment < -40)
-			increment = -40;
-	}
-	if (increment > 40)
-		increment = 40;
-
-	newprio = current->nice + increment;
-	if (newprio < -20)
-		newprio = -20;
-	if (newprio > 19)
-		newprio = 19;
-	current->nice = newprio;
-	return 0;
+    /*
+     *  Setpriority might change our priority at the same moment.
+     *  We don't have to worry. Conceptually one call occurs first
+     *  and we have a single winner.
+     */
+    if (increment < 0) {
+        if (!capable(CAP_SYS_NICE))
+            return -EPERM;
+        if (increment < -40)
+            increment = -40;
+    }
+    if (increment > 40)
+        increment = 40;
+
+    newprio = current->nice + increment;
+    if (newprio < -20)
+        newprio = -20;
+    if (newprio > 19)
+        newprio = 19;
+    current->nice = newprio;
+    return 0;
 }
 
 #endif
 
 static inline struct task_struct *find_process_by_pid(pid_t pid)
 {
-	struct task_struct *tsk = current;
+    struct task_struct *tsk = current;
 
-	if (pid)
-		tsk = find_task_by_pid(pid);
-	return tsk;
+    if (pid)
+        tsk = find_task_by_pid(pid);
+    return tsk;
 }
 
 static int setscheduler(pid_t pid, int policy, 
-			struct sched_param *param)
+            struct sched_param *param)
 {
-	struct sched_param lp;
-	struct task_struct *p;
-	int retval;
-
-	retval = -EINVAL;
-	if (!param || pid < 0)
-		goto out_nounlock;
-
-	retval = -EFAULT;
-	if (copy_from_user(&lp, param, sizeof(struct sched_param)))
-		goto out_nounlock;
-
-	/*
-	 * We play safe to avoid deadlocks.
-	 */
-	read_lock_irq(&tasklist_lock);
-	spin_lock(&runqueue_lock);
-
-	p = find_process_by_pid(pid);
-
-	retval = -ESRCH;
-	if (!p)
-		goto out_unlock;
-			
-	if (policy < 0)
-		policy = p->policy;
-	else {
-		retval = -EINVAL;
-		if (policy != SCHED_FIFO && policy != SCHED_RR &&
-				policy != SCHED_OTHER)
-			goto out_unlock;
-	}
-	
-	/*
-	 * Valid priorities for SCHED_FIFO and SCHED_RR are 1..99, valid
-	 * priority for SCHED_OTHER is 0.
-	 */
-	retval = -EINVAL;
-	if (lp.sched_priority < 0 || lp.sched_priority > 99)
-		goto out_unlock;
-	if ((policy == SCHED_OTHER) != (lp.sched_priority == 0))
-		goto out_unlock;
-
-	retval = -EPERM;
-	if ((policy == SCHED_FIFO || policy == SCHED_RR) && 
-	    !capable(CAP_SYS_NICE))
-		goto out_unlock;
-	if ((current->euid != p->euid) && (current->euid != p->uid) &&
-	    !capable(CAP_SYS_NICE))
-		goto out_unlock;
-
-	retval = 0;
-	p->policy = policy;
-	p->rt_priority = lp.sched_priority;
-	if (task_on_runqueue(p))
-		move_first_runqueue(p);
+    struct sched_param lp;
+    struct task_struct *p;
+    int retval;
+
+    retval = -EINVAL;
+    if (!param || pid < 0)
+        goto out_nounlock;
+
+    retval = -EFAULT;
+    if (copy_from_user(&lp, param, sizeof(struct sched_param)))
+        goto out_nounlock;
+
+    /*
+     * We play safe to avoid deadlocks.
+     */
+    read_lock_irq(&tasklist_lock);
+    spin_lock(&runqueue_lock);
+
+    p = find_process_by_pid(pid);
+
+    retval = -ESRCH;
+    if (!p)
+        goto out_unlock;
+            
+    if (policy < 0)
+        policy = p->policy;
+    else {
+        retval = -EINVAL;
+        if (policy != SCHED_FIFO && policy != SCHED_RR &&
+                policy != SCHED_OTHER)
+            goto out_unlock;
+    }
+    
+    /*
+     * Valid priorities for SCHED_FIFO and SCHED_RR are 1..99, valid
+     * priority for SCHED_OTHER is 0.
+     */
+    retval = -EINVAL;
+    if (lp.sched_priority < 0 || lp.sched_priority > 99)
+        goto out_unlock;
+    if ((policy == SCHED_OTHER) != (lp.sched_priority == 0))
+        goto out_unlock;
+
+    retval = -EPERM;
+    if ((policy == SCHED_FIFO || policy == SCHED_RR) && 
+        !capable(CAP_SYS_NICE))
+        goto out_unlock;
+    if ((current->euid != p->euid) && (current->euid != p->uid) &&
+        !capable(CAP_SYS_NICE))
+        goto out_unlock;
+
+    retval = 0;
+    p->policy = policy;
+    p->rt_priority = lp.sched_priority;
+    if (task_on_runqueue(p))
+        move_first_runqueue(p);
 
-	current->need_resched = 1;
+    current->need_resched = 1;
 
 out_unlock:
-	spin_unlock(&runqueue_lock);
-	read_unlock_irq(&tasklist_lock);
+    spin_unlock(&runqueue_lock);
+    read_unlock_irq(&tasklist_lock);
 
 out_nounlock:
-	return retval;
+    return retval;
 }
 
 asmlinkage long sys_sched_setscheduler(pid_t pid, int policy, 
-				      struct sched_param *param)
+                      struct sched_param *param)
 {
-	return setscheduler(pid, policy, param);
+    return setscheduler(pid, policy, param);
 }
 
 asmlinkage long sys_sched_setparam(pid_t pid, struct sched_param *param)
 {
-	return setscheduler(pid, -1, param);
+    return setscheduler(pid, -1, param);
 }
 
 asmlinkage long sys_sched_getscheduler(pid_t pid)
 {
-	struct task_struct *p;
-	int retval;
+    struct task_struct *p;
+    int retval;
 
-	retval = -EINVAL;
-	if (pid < 0)
-		goto out_nounlock;
-
-	retval = -ESRCH;
-	read_lock(&tasklist_lock);
-	p = find_process_by_pid(pid);
-	if (p)
-		retval = p->policy & ~SCHED_YIELD;
-	read_unlock(&tasklist_lock);
+    retval = -EINVAL;
+    if (pid < 0)
+        goto out_nounlock;
+
+    retval = -ESRCH;
+    read_lock(&tasklist_lock);
+    p = find_process_by_pid(pid);
+    if (p)
+        retval = p->policy & ~SCHED_YIELD;
+    read_unlock(&tasklist_lock);
 
 out_nounlock:
-	return retval;
+    return retval;
 }
 
 asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param *param)
 {
-	struct task_struct *p;
-	struct sched_param lp;
-	int retval;
-
-	retval = -EINVAL;
-	if (!param || pid < 0)
-		goto out_nounlock;
-
-	read_lock(&tasklist_lock);
-	p = find_process_by_pid(pid);
-	retval = -ESRCH;
-	if (!p)
-		goto out_unlock;
-	lp.sched_priority = p->rt_priority;
-	read_unlock(&tasklist_lock);
-
-	/*
-	 * This one might sleep, we cannot do it with a spinlock held ...
-	 */
-	retval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;
+    struct task_struct *p;
+    struct sched_param lp;
+    int retval;
+
+    retval = -EINVAL;
+    if (!param || pid < 0)
+        goto out_nounlock;
+
+    read_lock(&tasklist_lock);
+    p = find_process_by_pid(pid);
+    retval = -ESRCH;
+    if (!p)
+        goto out_unlock;
+    lp.sched_priority = p->rt_priority;
+    read_unlock(&tasklist_lock);
+
+    /*
+     * This one might sleep, we cannot do it with a spinlock held ...
+     */
+    retval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;
 
 out_nounlock:
-	return retval;
+    return retval;
 
 out_unlock:
-	read_unlock(&tasklist_lock);
-	return retval;
+    read_unlock(&tasklist_lock);
+    return retval;
 }
 
 asmlinkage long sys_sched_yield(void)
 {
-	/*
-	 * Trick. sched_yield() first counts the number of truly 
-	 * 'pending' runnable processes, then returns if it's
-	 * only the current processes. (This test does not have
-	 * to be atomic.) In threaded applications this optimization
-	 * gets triggered quite often.
-	 */
+    /*
+     * Trick. sched_yield() first counts the number of truly 
+     * 'pending' runnable processes, then returns if it's
+     * only the current processes. (This test does not have
+     * to be atomic.) In threaded applications this optimization
+     * gets triggered quite often.
+     */
 
-	int nr_pending = nr_running;
+    int nr_pending = nr_running;
 
 #if CONFIG_SMP
-	int i;
+    int i;
 
-	// Subtract non-idle processes running on other CPUs.
-	for (i = 0; i < smp_num_cpus; i++) {
-		int cpu = cpu_logical_map(i);
-		if (aligned_data[cpu].schedule_data.curr != idle_task(cpu))
-			nr_pending--;
-	}
+    // Subtract non-idle processes running on other CPUs.
+    for (i = 0; i < smp_num_cpus; i++) {
+        int cpu = cpu_logical_map(i);
+        if (aligned_data[cpu].schedule_data.curr != idle_task(cpu))
+            nr_pending--;
+    }
 #else
-	// on UP this process is on the runqueue as well
-	nr_pending--;
+    // on UP this process is on the runqueue as well
+    nr_pending--;
 #endif
-	if (nr_pending) {
-		/*
-		 * This process can only be rescheduled by us,
-		 * so this is safe without any locking.
-		 */
-		if (current->policy == SCHED_OTHER)
-			current->policy |= SCHED_YIELD;
-		current->need_resched = 1;
-
-		spin_lock_irq(&runqueue_lock);
-		move_last_runqueue(current);
-		spin_unlock_irq(&runqueue_lock);
-	}
-	return 0;
+    if (nr_pending) {
+        /*
+         * This process can only be rescheduled by us,
+         * so this is safe without any locking.
+         */
+        if (current->policy == SCHED_OTHER)
+            current->policy |= SCHED_YIELD;
+        current->need_resched = 1;
+
+        spin_lock_irq(&runqueue_lock);
+        move_last_runqueue(current);
+        spin_unlock_irq(&runqueue_lock);
+    }
+    return 0;
 }
 
 asmlinkage long sys_sched_get_priority_max(int policy)
 {
-	int ret = -EINVAL;
+    int ret = -EINVAL;
 
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = 99;
-		break;
-	case SCHED_OTHER:
-		ret = 0;
-		break;
-	}
-	return ret;
+    switch (policy) {
+    case SCHED_FIFO:
+    case SCHED_RR:
+        ret = 99;
+        break;
+    case SCHED_OTHER:
+        ret = 0;
+        break;
+    }
+    return ret;
 }
 
 asmlinkage long sys_sched_get_priority_min(int policy)
 {
-	int ret = -EINVAL;
+    int ret = -EINVAL;
 
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = 1;
-		break;
-	case SCHED_OTHER:
-		ret = 0;
-	}
-	return ret;
+    switch (policy) {
+    case SCHED_FIFO:
+    case SCHED_RR:
+        ret = 1;
+        break;
+    case SCHED_OTHER:
+        ret = 0;
+    }
+    return ret;
 }
 
 asmlinkage long sys_sched_rr_get_interval(pid_t pid, struct timespec *interval)
 {
-	struct timespec t;
-	struct task_struct *p;
-	int retval = -EINVAL;
-
-	if (pid < 0)
-		goto out_nounlock;
-
-	retval = -ESRCH;
-	read_lock(&tasklist_lock);
-	p = find_process_by_pid(pid);
-	if (p)
-		jiffies_to_timespec(p->policy & SCHED_FIFO ? 0 : NICE_TO_TICKS(p->nice),
-				    &t);
-	read_unlock(&tasklist_lock);
-	if (p)
-		retval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;
+    struct timespec t;
+    struct task_struct *p;
+    int retval = -EINVAL;
+
+    if (pid < 0)
+        goto out_nounlock;
+
+    retval = -ESRCH;
+    read_lock(&tasklist_lock);
+    p = find_process_by_pid(pid);
+    if (p)
+        jiffies_to_timespec(p->policy & SCHED_FIFO ? 0 : NICE_TO_TICKS(p->nice),
+                    &t);
+    read_unlock(&tasklist_lock);
+    if (p)
+        retval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;
 out_nounlock:
-	return retval;
+    return retval;
 }
 
 static void show_task(struct task_struct * p)
 {
-	unsigned long free = 0;
-	int state;
-	static const char * stat_nam[] = { "R", "S", "D", "Z", "T", "W" };
-
-	printk("%-13.13s ", p->comm);
-	state = p->state ? ffz(~p->state) + 1 : 0;
-	if (((unsigned) state) < sizeof(stat_nam)/sizeof(char *))
-		printk(stat_nam[state]);
-	else
-		printk(" ");
+    unsigned long free = 0;
+    int state;
+    static const char * stat_nam[] = { "R", "S", "D", "Z", "T", "W" };
+
+    printk("%-13.13s ", p->comm);
+    state = p->state ? ffz(~p->state) + 1 : 0;
+    if (((unsigned) state) < sizeof(stat_nam)/sizeof(char *))
+        printk(stat_nam[state]);
+    else
+        printk(" ");
 #if (BITS_PER_LONG == 32)
-	if (p == current)
-		printk(" current  ");
-	else
-		printk(" %08lX ", thread_saved_pc(&p->thread));
+    if (p == current)
+        printk(" current  ");
+    else
+        printk(" %08lX ", thread_saved_pc(&p->thread));
 #else
-	if (p == current)
-		printk("   current task   ");
-	else
-		printk(" %016lx ", thread_saved_pc(&p->thread));
+    if (p == current)
+        printk("   current task   ");
+    else
+        printk(" %016lx ", thread_saved_pc(&p->thread));
 #endif
-	{
-		unsigned long * n = (unsigned long *) (p+1);
-		while (!*n)
-			n++;
-		free = (unsigned long) n - (unsigned long)(p+1);
-	}
-	printk("%5lu %5d %6d ", free, p->pid, p->p_pptr->pid);
-	if (p->p_cptr)
-		printk("%5d ", p->p_cptr->pid);
-	else
-		printk("      ");
-	if (p->p_ysptr)
-		printk("%7d", p->p_ysptr->pid);
-	else
-		printk("       ");
-	if (p->p_osptr)
-		printk(" %5d", p->p_osptr->pid);
-	else
-		printk("      ");
-	if (!p->mm)
-		printk(" (L-TLB)\n");
-	else
-		printk(" (NOTLB)\n");
-
-	{
-		extern void show_trace_task(struct task_struct *tsk);
-		show_trace_task(p);
-	}
+    {
+        unsigned long * n = (unsigned long *) (p+1);
+        while (!*n)
+            n++;
+        free = (unsigned long) n - (unsigned long)(p+1);
+    }
+    printk("%5lu %5d %6d ", free, p->pid, p->p_pptr->pid);
+    if (p->p_cptr)
+        printk("%5d ", p->p_cptr->pid);
+    else
+        printk("      ");
+    if (p->p_ysptr)
+        printk("%7d", p->p_ysptr->pid);
+    else
+        printk("       ");
+    if (p->p_osptr)
+        printk(" %5d", p->p_osptr->pid);
+    else
+        printk("      ");
+    if (!p->mm)
+        printk(" (L-TLB)\n");
+    else
+        printk(" (NOTLB)\n");
+
+    {
+        extern void show_trace_task(struct task_struct *tsk);
+        show_trace_task(p);
+    }
 }
 
 char * render_sigset_t(sigset_t *set, char *buffer)
 {
-	int i = _NSIG, x;
-	do {
-		i -= 4, x = 0;
-		if (sigismember(set, i+1)) x |= 1;
-		if (sigismember(set, i+2)) x |= 2;
-		if (sigismember(set, i+3)) x |= 4;
-		if (sigismember(set, i+4)) x |= 8;
-		*buffer++ = (x < 10 ? '0' : 'a' - 10) + x;
-	} while (i >= 4);
-	*buffer = 0;
-	return buffer;
+    int i = _NSIG, x;
+    do {
+        i -= 4, x = 0;
+        if (sigismember(set, i+1)) x |= 1;
+        if (sigismember(set, i+2)) x |= 2;
+        if (sigismember(set, i+3)) x |= 4;
+        if (sigismember(set, i+4)) x |= 8;
+        *buffer++ = (x < 10 ? '0' : 'a' - 10) + x;
+    } while (i >= 4);
+    *buffer = 0;
+    return buffer;
 }
 
 void show_state(void)
 {
-	struct task_struct *p;
+    struct task_struct *p;
 
 #if (BITS_PER_LONG == 32)
-	printk("\n"
-	       "                         free                        sibling\n");
-	printk("  task             PC    stack   pid father child younger older\n");
+    printk("\n"
+           "                         free                        sibling\n");
+    printk("  task             PC    stack   pid father child younger older\n");
 #else
-	printk("\n"
-	       "                                 free                        sibling\n");
-	printk("  task                 PC        stack   pid father child younger older\n");
+    printk("\n"
+           "                                 free                        sibling\n");
+    printk("  task                 PC        stack   pid father child younger older\n");
 #endif
-	read_lock(&tasklist_lock);
-	for_each_task(p) {
-		/*
-		 * reset the NMI-timeout, listing all files on a slow
-		 * console might take alot of time:
-		 */
-		touch_nmi_watchdog();
-		show_task(p);
-	}
-	read_unlock(&tasklist_lock);
+    read_lock(&tasklist_lock);
+    for_each_task(p) {
+        /*
+         * reset the NMI-timeout, listing all files on a slow
+         * console might take alot of time:
+         */
+        touch_nmi_watchdog();
+        show_task(p);
+    }
+    read_unlock(&tasklist_lock);
 }
 
 /**
@@ -1231,114 +1256,114 @@
  */
 void reparent_to_init(void)
 {
-	struct task_struct *this_task = current;
+    struct task_struct *this_task = current;
 
-	write_lock_irq(&tasklist_lock);
+    write_lock_irq(&tasklist_lock);
 
-	/* Reparent to init */
-	REMOVE_LINKS(this_task);
-	this_task->p_pptr = child_reaper;
-	this_task->p_opptr = child_reaper;
-	SET_LINKS(this_task);
-
-	/* Set the exit signal to SIGCHLD so we signal init on exit */
-	this_task->exit_signal = SIGCHLD;
-
-	/* We also take the runqueue_lock while altering task fields
-	 * which affect scheduling decisions */
-	spin_lock(&runqueue_lock);
-
-	this_task->ptrace = 0;
-	this_task->nice = DEF_NICE;
-	this_task->policy = SCHED_OTHER;
-	/* cpus_allowed? */
-	/* rt_priority? */
-	/* signals? */
-	this_task->cap_effective = CAP_INIT_EFF_SET;
-	this_task->cap_inheritable = CAP_INIT_INH_SET;
-	this_task->cap_permitted = CAP_FULL_SET;
-	this_task->keep_capabilities = 0;
-	memcpy(this_task->rlim, init_task.rlim, sizeof(*(this_task->rlim)));
-	this_task->user = INIT_USER;
+    /* Reparent to init */
+    REMOVE_LINKS(this_task);
+    this_task->p_pptr = child_reaper;
+    this_task->p_opptr = child_reaper;
+    SET_LINKS(this_task);
+
+    /* Set the exit signal to SIGCHLD so we signal init on exit */
+    this_task->exit_signal = SIGCHLD;
+
+    /* We also take the runqueue_lock while altering task fields
+     * which affect scheduling decisions */
+    spin_lock(&runqueue_lock);
+
+    this_task->ptrace = 0;
+    this_task->nice = DEF_NICE;
+    this_task->policy = SCHED_OTHER;
+    /* cpus_allowed? */
+    /* rt_priority? */
+    /* signals? */
+    this_task->cap_effective = CAP_INIT_EFF_SET;
+    this_task->cap_inheritable = CAP_INIT_INH_SET;
+    this_task->cap_permitted = CAP_FULL_SET;
+    this_task->keep_capabilities = 0;
+    memcpy(this_task->rlim, init_task.rlim, sizeof(*(this_task->rlim)));
+    this_task->user = INIT_USER;
 
-	spin_unlock(&runqueue_lock);
-	write_unlock_irq(&tasklist_lock);
+    spin_unlock(&runqueue_lock);
+    write_unlock_irq(&tasklist_lock);
 }
 
 /*
- *	Put all the gunge required to become a kernel thread without
- *	attached user resources in one place where it belongs.
+ *  Put all the gunge required to become a kernel thread without
+ *  attached user resources in one place where it belongs.
  */
 
 void daemonize(void)
 {
-	struct fs_struct *fs;
+    struct fs_struct *fs;
 
 
-	/*
-	 * If we were started as result of loading a module, close all of the
-	 * user space pages.  We don't need them, and if we didn't close them
-	 * they would be locked into memory.
-	 */
-	exit_mm(current);
-
-	current->session = 1;
-	current->pgrp = 1;
-	current->tty = NULL;
-
-	/* Become as one with the init task */
-
-	exit_fs(current);	/* current->fs->count--; */
-	fs = init_task.fs;
-	current->fs = fs;
-	atomic_inc(&fs->count);
- 	exit_files(current);
-	current->files = init_task.files;
-	atomic_inc(&current->files->count);
+    /*
+     * If we were started as result of loading a module, close all of the
+     * user space pages.  We don't need them, and if we didn't close them
+     * they would be locked into memory.
+     */
+    exit_mm(current);
+
+    current->session = 1;
+    current->pgrp = 1;
+    current->tty = NULL;
+
+    /* Become as one with the init task */
+
+    exit_fs(current);   /* current->fs->count--; */
+    fs = init_task.fs;
+    current->fs = fs;
+    atomic_inc(&fs->count);
+    exit_files(current);
+    current->files = init_task.files;
+    atomic_inc(&current->files->count);
 }
 
 extern unsigned long wait_init_idle;
 
 void __init init_idle(void)
 {
-	struct schedule_data * sched_data;
-	sched_data = &aligned_data[smp_processor_id()].schedule_data;
+    struct schedule_data * sched_data;
+    sched_data = &aligned_data[smp_processor_id()].schedule_data;
 
-	if (current != &init_task && task_on_runqueue(current)) {
-		printk("UGH! (%d:%d) was on the runqueue, removing.\n",
-			smp_processor_id(), current->pid);
-		del_from_runqueue(current);
-	}
-	sched_data->curr = current;
-	sched_data->last_schedule = get_cycles();
-	clear_bit(current->processor, &wait_init_idle);
+    if (current != &init_task && task_on_runqueue(current)) {
+        printk("UGH! (%d:%d) was on the runqueue, removing.\n",
+            smp_processor_id(), current->pid);
+        del_from_runqueue(current);
+    }
+    sched_data->curr = current;
+    sched_data->last_schedule = get_cycles();
+    clear_bit(current->processor, &wait_init_idle);
 }
 
 extern void init_timervecs (void);
 
 void __init sched_init(void)
 {
-	/*
-	 * We have to do a little magic to get the first
-	 * process right in SMP mode.
-	 */
-	int cpu = smp_processor_id();
-	int nr;
-
-	init_task.processor = cpu;
-
-	for(nr = 0; nr < PIDHASH_SZ; nr++)
-		pidhash[nr] = NULL;
-
-	init_timervecs();
-
-	init_bh(TIMER_BH, timer_bh);
-	init_bh(TQUEUE_BH, tqueue_bh);
-	init_bh(IMMEDIATE_BH, immediate_bh);
-
-	/*
-	 * The boot idle thread does lazy MMU switching as well:
-	 */
-	atomic_inc(&init_mm.mm_count);
-	enter_lazy_tlb(&init_mm, current, cpu);
+    /*
+     * We have to do a little magic to get the first
+     * process right in SMP mode.
+     */
+    int cpu = smp_processor_id();
+    int nr;
+
+    init_task.processor = cpu;
+
+    for(nr = 0; nr < PIDHASH_SZ; nr++)
+        pidhash[nr] = NULL;
+
+    init_timervecs();
+
+    init_bh(TIMER_BH, timer_bh);
+    init_bh(TQUEUE_BH, tqueue_bh);
+    init_bh(IMMEDIATE_BH, immediate_bh);
+
+    /*
+     * The boot idle thread does lazy MMU switching as well:
+     */
+    atomic_inc(&init_mm.mm_count);
+    enter_lazy_tlb(&init_mm, current, cpu);
 }
